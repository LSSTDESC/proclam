{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![](./header.png) -->\n",
    "<img src=\"./header.png\",width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC): code that runs the performance metric\n",
    "\n",
    "*Alex Malz (NYU)*, *Renee Hlozek (U. Toronto)*, *Tarek Alam (UCL)*, *Anita Bahmanyar (U. Toronto)*, *Rahul Biswas (U. Stockholm)*, *Rafael Martinez-Galarza (Harvard)*, *Gautham Narayan (STScI)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['figure.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This is the code available on GitHub for calculating metrics,\n",
    "# as well as performing other diagnostics on probability tables.\n",
    "\n",
    "import proclam\n",
    "from proclam import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all functions for preprocessing Renee's data files.\n",
    "\n",
    "def make_class_pairs(data_info_dict):\n",
    "    for name in data_info_dict['names']:\n",
    "        data_info_dict['class_pairs'][name] = [data_info_dict['classifications'][name], data_info_dict['truth_tables'][name]]\n",
    "    return(data_info_dict['class_pairs'])\n",
    "        \n",
    "def make_file_locs(data_info_dict):\n",
    "    names = data_info_dict['names']\n",
    "    data_info_dict['dirname'] = topdir + data_info_dict['label'] + '/'\n",
    "    for name in names:\n",
    "        data_info_dict['classifications'][name] = '%s/predicted_prob_%s.csv'%(name, name)\n",
    "        data_info_dict['truth_tables'][name] = '%s/truth_table_%s.csv'%(name, name)\n",
    "    return data_info_dict\n",
    "\n",
    "def process_strings(dataset, cc):\n",
    "    loc = dataset['dirname']\n",
    "    text = dataset['label'] + ' ' + dataset['names'][cc]\n",
    "    return loc, text\n",
    "\n",
    "def just_read_class_pairs(pair, dataset, cc):\n",
    "    loc, text = process_strings(dataset, cc)\n",
    "    clfile = pair[0]\n",
    "    truthfile = pair[1]\n",
    "    prob_mat = pd.read_csv(loc + clfile, delim_whitespace=True).values\n",
    "    nobj = np.shape(prob_mat)[0]\n",
    "    nclass = np.shape(prob_mat)[1]\n",
    "    truth_values = pd.read_csv(loc + truthfile, delim_whitespace=True).values\n",
    "    nobj_truth = np.shape(truth_values)[0]\n",
    "    nclass_truth = np.shape(truth_values)[1]\n",
    "    tvec = np.where(truth_values==1)[1]\n",
    "    pmat = prob_mat\n",
    "    return pmat, tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renee gave me this data about which I know nothing.\n",
    "mystery = {}\n",
    "mystery['label'] = 'Unknown'\n",
    "mystery['names'] = ['RandomForest', 'KNeighbors', 'MLPNeuralNet']\n",
    "mystery['classifications'] = {}\n",
    "mystery['truth_tables'] = {}\n",
    "mystery['class_pairs'] = {}\n",
    "mystery['probs'] = {}\n",
    "mystery['truth'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're just preprocessing Renee's data here.\n",
    "\n",
    "topdir = '../examples/'\n",
    "mystery = make_file_locs(mystery)\n",
    "mystery['class_pairs'] = make_class_pairs(mystery)\n",
    "for nm, name in enumerate(mystery['names']):\n",
    "    probm, truthv = just_read_class_pairs(mystery['class_pairs'][name], mystery, nm)\n",
    "    mystery['probs'][name] = probm\n",
    "    mystery['truth'][name] = truthv\n",
    "M_classes = np.shape(probm)[-1]\n",
    "\n",
    "# we need the class labels in the dataset in a consistently sorted order \n",
    "# and will assume the weights of the weightvec correspond to this order\n",
    "class_labels = sorted(np.unique(mystery['truth']['RandomForest']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method (Metric)\n",
    "======\n",
    "\n",
    "The log-loss is defined as\n",
    "\\begin{eqnarray*}\n",
    "L &=& -\\sum_{m=1}^{M}\\frac{w_{m}}{N_{m}}\\sum_{n=1}^{N_{m}}\\ln[p_{n}(m | m)]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We calculate the metric within each class $m$ by taking an average of its value $-\\ln[p_{n}(m | m)]$ for each true member  $n$ of the class.  Then we weight the metrics for each class by an arbitrary weight $w_{m}$ and take a weighted average of the per-class metrics to produce a global scalar metric $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you run the metric with a random weight vector.\n",
    "\n",
    "metric = 'LogLoss'\n",
    "weightvec = np.ones(M_classes)\n",
    "# dummy example for SNPhotCC demo data\n",
    "special_classes = (1, 10, 11, 12)\n",
    "# we should be using this for the PLAsTiCC data\n",
    "# special_clases = (51, 99)\n",
    "mask = np.array([True if classname in special_classes else False for classname in class_labels])\n",
    "weightvec[mask] = 2\n",
    "weightvec = weightvec / sum(weightvec)\n",
    "name = np.random.choice(mystery['names'])\n",
    "probm = mystery['probs'][name]\n",
    "truthv = mystery['truth'][name]\n",
    "LL = getattr(proclam.metrics, metric)()\n",
    "val = LL.evaluate(prediction=probm, truth=truthv, averaging=weightvec)\n",
    "print(name+' with weights '+str(weightvec)+' has '+metric+' = '+str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgments\n",
    "===============\n",
    "\n",
    "The DESC acknowledges ongoing support from the Institut National de Physique Nucleaire et de Physique des Particules in France; the Science & Technology Facilities Council in the United Kingdom; and the Department of Energy, the National Science Foundation, and the LSST Corporation in the United States.\n",
    "\n",
    "DESC uses resources of the IN2P3 Computing Center (CC-IN2P3--Lyon/Villeurbanne - France) funded by the Centre National de la Recherche Scientifique; the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231; STFC DiRAC HPC Facilities, funded by UK BIS National E-infrastructure capital grants; and the UK particle physics grid, supported by the GridPP Collaboration.\n",
    "\n",
    "This work was performed in part under DOE Contract DE-AC02-76SF00515."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributions\n",
    "=======\n",
    "\n",
    "Alex Malz: conceptualization, data curation, formal analysis, investigation, methodology, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Renee Hlozek: data curation, formal analysis, funding acquisition, investigation, project administration, software, supervision, validation, visualization, writing - original draft\n",
    "\n",
    "Tarek Alam: investigation, software, validation\n",
    "\n",
    "Anita Bahmanyar: formal analysis, investigation, methodology, software, writing - original draft\n",
    "\n",
    "Rahul Biswas: conceptualization, methodology, software\n",
    "\n",
    "Rafael Martinez-Galarza: data curation, software, visualization\n",
    "\n",
    "Gautham Narayan: data curation, formal analysis"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (proclam)",
   "language": "python",
   "name": "proclam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
