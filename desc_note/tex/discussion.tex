\section{Discussion}
\label{sec:discussion}

The metrics investigated here are specific to the first version of \plasticc, in which participants compete to classify lightcurves comparable to those that will be available at the end of LSST's main mission.
However, most transient and variable classes considered here would benefit from classification well before that time, even within the first days after an alert is triggered, largely for the sake of prompt spectroscopic follow-up.
Additionally, there are other perspectives on what constitutes a good classifier, and those perspectives must be boiled down to a metric in order to identify a classifier that is best for the task.
For this reason, there will likely be future versions of \plasticc, each requiring a metric appropriate to its goals.
We proceed to discuss approaches to identifying optimal metrics for these variations.

\subsection{Early classification}
\label{sec:early}

Resources for spectroscopic follow-up are limited, to the point that spectroscopic follow-up is only expected of \aim{X} of \lsst's anticipated \aim{Y} detected transients and variables.
Prompt follow-up is more valuable for the purposes of both fitting models to the lightcurves of familiar objects and characterizing anomalous lightcurves that could be signatures of never-before-seen classes that have evaded identification due to rarity or faintness.
As such, decisions about follow-up must be made both quickly and without exceeding the budget of available resources.
We note that the decision of whether to initiate follow-up observations is binary and deterministic, so a probabilistic classification would ultimately be reduced to a deterministic one for this application.

Even within the scope of spectroscopic follow-up as a primary motivation for early lightcurve classification, these two goals would likely not share a preferred metric.
One might ask if there is a way to systematically select the optimal metric for a science goal, and the answer is to maximize information.
The reason the metrics ought to differ is that the science goals benefit from information about different things.

In SN cosmology, each true positive adds one high-fidelity datapoint to the Hubble diagram, thereby constraining the cosmological parameters, so only true positives contribute information, and if we had a perfect classifier and standard follow-up spectroscopy resources, there would be a maximum amount of information about the cosmological parameters that could be gained in this way.
Each false positive uses the same resources but adds no information about the cosmological parameters, and each false negative consumes no follow-up resources and deprives the Hubble diagram of one more data point.
For SN Ia, the metric must be chosen to balance the value of the information forgone by a false positive and the value of information forgone by a false negative, and the value placed on these is effectively weighted by the value we as researchers place on follow-up resources.
When our objects are as plentiful as SN Ia and our resources limited a priori, we may not be concerned by a high rate of false negatives so long as our false positive rate is minimized, because each false positive actually reduces the number of true positives available to contribute information.

Anomaly detection also gains information only from true positives, but the cost function is different for this case because the potential gain of information from a perfect classifier is unbounded.
In this case, the value of the information forgone by a false negative is significant compared to the value of the information forgone by a false positive.
For a rare event like a kilonova, a false negative does not appreciably reduce the amount of remaining information available to collect, but a false positive represents a large quantity of information forgone.
Thus, a metric tuned to anomaly detection would aim to minimize the false positive rate more strongly than the false negative ratel.

\subsection{Hierarchical classes}
\label{sec:hierarchical}

\aim{Discuss conditional extensions of log-loss.}

\subsection{Difficult lightcurve classification}
\label{sec:difficult}

It may be desirable to weight metrics by the difficulty of the classification, i.e. correctly classifying the noisiest lightcurves should be rewarded more than correctly classifying the cleanest, easiest lightcurves.
The groundwork for such a metric has been laid by Wu+18.
\aim{Still under construction, requesting help from Rahul Biswas.}
