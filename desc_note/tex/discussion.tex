\section{Discussion}
\label{sec:discussion}
The metrics investigated here are specific to the first version of \plasticc, in which participants compete to classify complete lightcurves similar to light curves in LSST data releases. A slightly different and interesting problem which might be part of later versions of \plasticc is the early classification of events, which is discussed in the subsection \ref{sec:early}, and the investigation of metrics for that problem will be addressed in a separate work.  
Additionally, there are other perspectives on what constitutes a good classifier, and those perspectives must be boiled down to a metric in order to identify a classifier that is best for the task.
For this reason, there will likely be future versions of \plasticc, each requiring a metric appropriate to its goals.
We proceed to discuss approaches to identifying optimal metrics for these variations.

\subsection{Early classification}
\label{sec:early}

Resources for spectroscopic follow-up are limited, to the point that spectroscopic follow-up is only expected of a fraction of detected transients and variables, with resources focused towards objects already flagged as novel and interesting (by a classifier), or useful for cosmology, again using classification algorithms on photometric data.
Prompt follow-up is more valuable for the purposes of both fitting models to the lightcurves of familiar objects and characterizing anomalous lightcurves that could be signatures of never-before-seen classes that have evaded identification due to rarity or faintness.
As such, decisions about follow-up must be made both quickly and without exceeding the budget of available resources.
We note that the decision of whether to initiate follow-up observations is binary and deterministic, so a probabilistic classification would ultimately be reduced to a deterministic one for this application.

Even within the scope of spectroscopic follow-up as a primary motivation for early lightcurve classification, these two goals would likely not share a preferred metric.
One might ask if there is a way to systematically select the optimal metric for a science goal, and the answer is to maximize information.
The reason the metrics ought to differ is that the science goals benefit from information about different things.

In SN cosmology, each true positive adds one high-fidelity datapoint to the Hubble diagram, thereby constraining the cosmological parameters, so only true positives contribute information, and if we had a perfect classifier and standard follow-up spectroscopy resources, there would be a maximum amount of information about the cosmological parameters that could be gained in this way.
Each false positive uses the same resources but adds no information about the cosmological parameters, and each false negative consumes no follow-up resources and deprives the Hubble diagram of one more data point.
For SN Ia, the metric must be chosen to balance the value of the information forgone by a false positive and the value of information forgone by a false negative, and the value placed on these is effectively weighted by the value we as researchers place on follow-up resources.
When our objects are as plentiful as SN Ia and our resources limited a priori, we may not be concerned by a high rate of false negatives so long as our false positive rate is minimized, because each false positive actually reduces the number of true positives available to contribute information.

Anomaly detection also gains information only from true positives, but the cost function is different for this case because the potential gain of information from a perfect classifier is unbounded.
In this case, the value of the information forgone by a false negative is significant compared to the value of the information forgone by a false positive.
For a rare event like a kilonova, a false negative does not appreciably reduce the amount of remaining information available to collect, but a false positive represents a large quantity of information forgone.
Thus, a metric tuned to anomaly detection would aim to minimize the false positive rate more strongly than the false negative ratel.

%\subsection{Hierarchical classes}
%\label{sec:hierarchical}
%\aim{Discuss conditional extensions of log-loss.}

\subsection{Difficult lightcurve classification}
\label{sec:difficult}

In any astronomical survey, most of the detected objects in a particular class will be distant and therefore dim leading to poor signal to noise and data quality. While the use of such an object for a scientific purpose (eg. for SN cosmology, a very low SNR light curve would produce a low quality distance measurement), this could possibly be overcome by the sheer number of such low SNR light curves. Thus, classification methods which
work well on low quality light curves are specially important for opening up the possibilities of using such objects in science. While the groundwork for producing a metric has been laid by \cite{wu_radio_2018}). In this work, we will not discuss metrics that take quality into account, but assume (and we know, this is true for \plasticc )
 selection cuts to have cut out the largest  sample of poor light curves. In a later work, we hope to return to the possibility of including this in the metric itself, so that such selection cuts are unnecessary.

%\aim{Still under construction, requesting help from Rahul Biswas.: Added some, feel free to add / subtract}
