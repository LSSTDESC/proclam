\section{Results}
\label{sec:results}
In order to understand how a given classifier with its specific properties (e.g. those that take a `winner takes all' approach compared to those that provide balanced classifications across types) performs in different metric schemes, we simulate the responses of both the log-loss metric and the Brier score as a function of isolated `systematic effects' imposed on various classifier baselines. What we mean by systematic is some alteration to the standard classifier that could affect its performance.  We outline these systematics below.

\subsection{Mock classifier systematics}
\label{sec:mockresults}
In Figure~\ref{fig:cruise}, we illustrate shows the effect of adding each of four forms of error to a classification scheme that performed perfectly at classifying objects of a given type (prior to the systematic being added). The four systematics include changing the classification from perfect to almost perfect as described in Section~\label{sec:accuratedata},  subsuming the classifications of class $m'$ for another class $\tilde{m}$ (labelled by `subsumed'), changing the classification from perfect to wholly uncertain, and making the classification more `noisy', which is by a reduction in the contribution of the classifier by a factor of $s=2,$ as described in Section \label{sec:accuratedata}.

In each case, the metric weights are assumed to be flat.  The log-loss and Brier scores behave very consistently, with the log-loss more affected than the Brier score for systematics in which the class $m'$ is subsumed by a different class (top right panel), however the effect is reasonably small. This shows that for a given metric (log-loss/Brier) and classification scheme, the metrics are stable to the introduction of systematic changes in the classifications.
Both metrics are appropriate for singling out a classifier that enables any class to subsume other classes, thereby confirming that the cruise control classifier will not in general outscore a classifier that avoids subsuming.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{./fig/systematics_onlyperfect.png}
		\caption{We confirm that the addition of systematics to the perfect classifier increases the metric values and that the increase is more or less proportional to our concerns about the systematic. \textbf{RH:green text is not visible.}
		\label{fig:cruise}}
	\end{center}
\end{figure}

\aim{Preliminary results indicate weighting will be very important for preventing the tunnel vision classifier from winning.
It may be necessary to a priori anticipate which classes will have to be most strongly protected from this systematic via upweighting them.}

%\begin{figure}
%	\begin{center}
%		% \includegraphics[width=0.5\textwidth]{./fig/systematics_onlyperfect.png}
%		\caption{\aim{After much iteration on how best to present these tests, a figure similar to Figure~\ref{fig:cruise} but for the tunnel vision classifier (heading) on different baseline classifications (panels) as a function of weight on the affected class (rather than number of classes) is under construction.}}
%		\label{fig:tunnel}
%	\end{center}
%\end{figure}

\aim{For reference, Figure~\ref{fig:popweight} shows the effect of weighting by true class population.}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{./fig/all_effects_isolated.png}
		\caption{\aim{The tunnel vision classifier is hard to beat!}}
		\label{fig:popweight}
	\end{center}
\end{figure}



\subsection{Representative classifications}
\label{sec:realresults}

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.85\textwidth]{./fig/SNPhotCC_res.png}
		% \includegraphics[width=0.45\textwidth]{./fig/Unknown_res.png}
		\caption{equal weight per class -- what kinds of weightings are meaningful to test here?  Would it be reasonable to take the real SNPhotCC entries' confusion matrices and see who would have won under each weighting/metric combination?}
		\label{fig:real_metric_compare}
	\end{center}
\end{figure*}

\textbf{Current plan is that we compute the SNPhotCC metric for Michelle's contributions and also turn the binary classifications into probabilities - Renee will flesh this out}
\aim{We are still iterating on the most informative tests to conduct on the \snphotcc\ data.
I would like to have the confusion matrices from a real classification challenge (\snphotcc\ or Ashish Mahabal's) and test different weightings of our metrics on mock classification probabilities derived from those confusion matrices to check whether we choose the same ``winner,'' but the arrangements have not yet been finalized.
The ``pipeline,'' however, is complete and ready to run as soon as the test conditions are agreed upon.}


% \subsection{Weighting systematics}
% \label{sec:weight_res}
%
% \aim{We have not yet reached consensus on what tests are reasonable in the absence of physical motivations, as the test cases here are independent of such context.}
