\section{Data}
\label{sec:data}

We explore the behavior of the metrics on toy data with well-understood systematics as well as realistic mock data from past classification challenges.
Data is in the form of catalogs of posterior probability vectors $p(m \mid d)$ over $M$ classes $m$ conditioned on the observed data $d$, with each probability vector normalized to sum to unity.
We introduce the convention that the M$^{\mathrm{th}}$ class is designated ``other'' to encompass never-before-seen classes.

\subsection{Mock classifier systematics}
\label{sec:mockdata}

The test cases of this section are devised to confirm that our metric aligns with our intuitive understanding of what constitutes a good classifier, that it should not reward classifications suffering from the systematics that we find most concerning.
We consider a situation with $M=13$ classes, with one designated as ``other,'' and with a lognormal distribution of the number of members of each class.
We test eight mock classifiers, each of which derives classification probabilities based on the true classes, as a proxy for the information contained in the data, and a confusion matrix, encapsulating the systematic errors we wish to test.
The probability vector for a given object is a perturbation of the row of the confusion matrix corresponding to its true class, with a perturbation factor of $\delta=0.1$.
\aim{Write out an equation showing how $\delta$ factors in.}
Figure~\ref{fig:mock_cm} shows the confusion matrices corresponding to each error as well as four experimental controls.
For each case, we address:
\begin{enumerate}
  \item What defines this systematic?
  \item When has this systematic been observed before?
  \item What are our expectations for this systematic's effect on metric behavior?
\end{enumerate}

\begin{figure*}
	\begin{center}
    \includegraphics[width=0.8\textwidth]{./fig/all_sim_cm.png}
		\caption{leftmost top: uniform confusion matrix,
    leftmost bottom: perfect classifications,
    left-center top: unbiased classifications with uniform 10\% uncertainty,
    left-center bottom: unbiased classifications with uniform 50\% uncertainty,
    right-center top: perfect classification (type 1 and type 2 errors) for one class and uniform for all others,
    right-center bottom: assigning all objects the same class apart from 10\% uniform error,
    rightmost top: consistently assign one class to another,
    rightmost bottom: consistently assign another class to one class}
		\label{fig:mock_cm}
	\end{center}
\end{figure*}

\subsubsection{Agnostic classifier}
\label{sec:agnostic_data}

The confusion matrix of the ``Uncertain'' classifier is entirely uniform.
The deterministic classifications corresponding to this classifier are uniform random guesses, but the classification probability vectors drawn from it are perturbations of flat probability vectors whose peak values are uniformly distributed.
This classifier is an experimental control for the worst possible classifier in the absence of systematic biases, noting that if classifications were anticorrelated with true classes, the experimenter would simply relabel to improve performance.

\subsubsection{Perfect classifier}
\label{sec:perfect_data}

The ``Perfect'' classifier classifies all classes correctly.
This means that the probability of all the diagonal elements of the confusion matrix are 1 while the off-diagonal elements are all 0.

\subsubsection{Almost perfect classifier}
\label{sec:almost_data}

This is a similar classifier as the perfect classifier in the sense that it classifies all the classes correctly but there is some uncertainty in classification.
Therefore, the matrix has larger probabilities on the diagonal elements but they are smaller than 1 and all the off-diagonal elements have very small values.

\subsubsection{Noisy classifier}
\label{sec:nois_datay}

 \aim{I am not sure how to  make the distinction between "almost perfcet classifier" and "noisy classifier".
 Is there a threshold for the probability values to distinguish between the two?}

\subsubsection{Tunnel vision classifier}
\label{sec:tunnel_data}

This classifier works well for only one class.
This means that the probability given to the correct class for one specific class is high while the probabilities for all the other classes are randomly assigned meaning that it only classifies that specific class correctly.
This is shown in panel c of Figure \ref{fig:mock_cm} where only class 4 has been classified correctly as it has a high value on the diagonal element, while all the other elements have low probabilities.

\subsubsection{Cruise control classifier}
\label{sec:cruise_data}

This classifier classifies all the objects as one specific class.
This might happen when we have a lot of data in one class while the number of training data in other classes are very small so the classifier does not have enough data to be trained on and assigns all the test data as the single class it has a lot of data.
This might happen to transients that are rare and we do not observed any of them yet \aim{(this is only my idea so might not be true)}

\subsubsection{Subsuming classifier}
\label{sec:subsume_data}

This classifier classifies class m as class m' all the time.
This is shown in panel 8 of Figure \ref{fig:mock_cm}  where all classes are classified correctly where they have higher values on the diagonal element except one class that has higher probability on a non-diagonal component which shows that specific class has been classified as the other class.
This could happen when two classes of data are very similar but only different in a small property and the data that we have is not representative of that differentiating property so they both get classified as one similar class. \aim{(this is also what I think and might not be true)}

\subsubsection{Combination of systematics}
\label{sec:combo}

\begin{figure}
	\begin{center}
		% \includegraphics[width=0.45\textwidth]{./fig/Combined.png}
		\caption{}
		\label{fig:combo_cm}
	\end{center}
\end{figure}

\subsection{Representative classifications}
\label{sec:realdata}

\aim{This discussion probably belongs in Section~\ref{sec:methods} because it's about the metrics used in SNPhotCC, not the systematics in the submitted classification results.}

\aim{It's unclear how the three-class case of SNPhotCC is relevant.
We should perhaps do this for Ashish's confusion matrices for more classes.}

Classification problems in astronomy have historically focused on separating a heterogenous population into a limited number of subclasses (cite Kessler+2010), with the focus or goal being to identify one particular type of object.

In the Supernova Photometric Classification Challenge (SNPhotCC), the metric for deciding on who `won' the challenge was determined as a ratio of the efficiency of Type Ia classification and a `pseudo purity' factor, and a penalty for false-flagging (which is related to the cost of following up an object spectroscopically that is not a SNIa).
The figure of merit was given by:

\begin{eqnarray}
\mathcal{C}_{FOM-Ia} &\equiv& \frac{1}{\mathcal{N}_{Ia}^{TOT}}\times \frac{(N_{Ia}^{\mathrm{true}})^2}{N_{Ia}^\mathrm{true}+W_{Ia}^\mathrm{false}N_{Ia}^\mathrm{false}}
\end{eqnarray}
%\aim{Renee Hlozek will write the descriptions of these datasets.}

The above reduces to $\mathcal{C}_{FOM-Ia}  = \epsilon_{Ia} + PP_{Ia},$ the efficiency and pseudopurity, which can be interpreted as the traditional purity factor in the limit that the weight $W_{Ia}^\mathrm{false} = 1$.
For the SNPhotCC the false penalty was related to the size of the spectroscopic subsample as roughly $W_{Ia}^\mathrm{false} = 1 + \epsilon_{spec}^{-1} \gg 1$ but the conservative limit of $W_{Ia}^\mathrm{false} = 3$ was chosen to penalize wasted spectroscopic time over rejected SNe.

For future challenges, a more balanced metric can be used to ensure correct classifications across the range of objects, without focusing or highlighting a specific object as above.
Furthermore,

In response to the SNPhotCC, a range of classifications approaches were submitted, namely $\chi^{2}$ fits of the SN data to publicly available templates (ref. Nugent+), identification with the SiFTO light curve fitter (Conley et al. 2008) through the fitted values of the stretch, colour and also the $\chi^{2}$ of the fit for Ia fits, and a linear slope to magnitudes per day was used for the non-Ia sample.
Other groups constructed a Hubble diagram from the training data, and used the residuals away from that Hubble diagram as a criterion for classification in the test data.
Still other groups performed classification by reducing the dimensionality of the light curves (the so-called InCA approach).
A general light curve shape (rather than one motivated by the physical differences between SNeIa and core collapse SNe) was assumed by some competitors and then a kernel density estimation was performed over the fit parameters, with various approaches employed including boosting over the feature space.

Machine learning was also eployed over the reduced data set of light-curve slopes (in magnitudes per day) as above to produce a predictive model for the training data.
For more information on these methods and their success within the SNPhotCC, we refer the reader to Kessler+2010.

The above approaches vary between very physically motivated and \textbf{template based} (and also prone to bias given non-representativity of the test data) and agnostic and based on decomposition of the light curves into \textbf{generic features} (but also making use of less information in the classifications).
In figure~\ref{fig:snphotcc_cm} we show a range of classification approaches over the wavelet (feature) space (top row) and using templates (bottom row).

\begin{figure*}
	\begin{center}
    \includegraphics[width=\textwidth]{./fig/all_snphotcc_cm.png}
		\caption{SNPhotCC confusion matrices; note that the KNN classifiers suffer from the cruise control systematic, }
		\label{fig:snphotcc_cm}
	\end{center}
\end{figure*}



% \subsubsection{Unknown dataset}
% \label{sec:mystery}
%
% \begin{figure*}
% 	\begin{center}
% 		\includegraphics[width=0.3\textwidth]{./fig/Unknown_MLPNeuralNet_cm.png}
% 		\includegraphics[width=0.3\textwidth]{./fig/Unknown_KNeighbors_cm.png}
% 		\includegraphics[width=0.3\textwidth]{./fig/Unknown_RandomForest_cm.png}
% 		\caption{}
% 		\label{fig:unknown_cm}
% 	\end{center}
% \end{figure*}
