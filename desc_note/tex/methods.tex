\section{Methods}
\label{sec:methods}

%\begin{itemize}
%\item    The metric must return a single scalar value.
%\item    The metric must be well-defined for non-binary classes.
%\item    The metric must balance diverse science use cases in the presence of heavily nonuniform class prevalence.
%\item    The metric must respect the information content of probabilistic classifications.
%\item    The metric must be able to evaluate deterministic classifications.
%\item    The metric must be interpretable, meaning it gives a more optimal value for "good" mock classifiers and a less optimal value for mock classifiers plagued by anticipated systematic errors; in other words, it must pass basic tests of intuition.
%\item    The metric must be reliable, giving consistent results for different instantiations of the same test case.
%\end{itemize}

\plasticc\ aims to identify the most promising classification techniques, meaning there must be a performance metric, a single scalar value quantifying how appropriate a classifier is for the task at hand.
Choosing a metric therefore is logically entwined with the question the challenge aims to answer.
\snphotcc, for example, sought the best classifier of SN Ia, for the purpose of devoting precious follow-up resources to spectroscopically confirm likely candidates and use the spectroscopic data for cosmology investigations, and from that specific goal a well-defined metric followed.
The choice of a metric is not so simple for \plasticc, which has many science goals and the reality that most analyses using \lsst\ lightcurves will be done without follow-up data.
The probability densities classifiers produce must be accurate for use in inference as well as for the allocation of follow-up resources, and they must be evaluated in a way that does not excessively favor any one science application.

In Section~\ref{sec:past}, we review the metrics that have been used in previous challenges with deterministic classifications.
In Section~\ref{sec:metrics}, we introduce metrics appropriate for probabilistic classifications.
We take weighted averages of the per-object metrics with per-class weights described in Section~\ref{sec:weights}.
% Section~\ref{sec:inception} discusses the systematic way in which we evaluate the performance of the metrics.

\subsection{Traditional metrics}
\label{sec:past}

Classification problems in astronomy have historically focused on separating a heterogenous population into a limited number of subclasses (cite Kessler+2010), with the focus or goal being to identify one particular type of object.
The Supernova Photometric Classification Challenge (SNPhotCC) aimed to identify deterministic classifiers of SN Ia using a metric of the ratio of the efficiency of SN Ia classification and a `pseudo purity' factor with an additive penalty term for false positives (which is related to the cost of following up an object spectroscopically that is not a SNIa).
The figure of metric was given by
\begin{eqnarray}
  \label{eq:snphotcc}
  \mathcal{C}_{FOM-Ia} &\equiv& \frac{1}{\mathcal{N}_{Ia}^{TOT}}\times \frac{(N_{Ia}^{\mathrm{true}})^2}{N_{Ia}^\mathrm{true}+W_{Ia}^\mathrm{false}N_{Ia}^\mathrm{false}}.
\end{eqnarray}
%\aim{Renee, what is this in terms of the notation of the rest of the paper, or, at minimum, in terms of true/false positive/negative?}

The above reduces to $\mathcal{C}_{FOM-Ia}  = \epsilon_{Ia} + PP_{Ia},$ the efficiency and pseudopurity, which can be interpreted as the traditional purity factor in the limit that the weight $W_{Ia}^\mathrm{false} = 1$.
The efficiency is the ratio of the number of true postives successfully recovered as the fraction of the true classifications or TP/TP+FN, while the purity can itself be related tothe ratio of TP/TP+FP, or the positive predictive valuev.

For the SNPhotCC the false penalty was related to the size of the spectroscopic subsample as roughly $W_{Ia}^\mathrm{false} = 1 + \epsilon_{spec}^{-1} \gg 1$ but the conservative limit of $W_{Ia}^\mathrm{false} = 3$ was chosen to penalize wasted spectroscopic time over rejected SNe.

For future challenges, a more balanced metric can be used to ensure correct classifications across the range of classes, without focusing or highlighting a specific class as above.

\subsection{Probabilistic metrics}
\label{sec:metrics}

\plasticc\ aims to identify classifiers that produce discrete posterior probability distributions $p(m \mid d)$ over $M$ classes $m$ given their photometric lightcurve data $d$.

We consider two  metrics of classification probabilities, each of which is interpretable and avoids reducing probabilities to point estimates.
Both our metrics make use of the indicator variable $\tau_{n, m}$ \changes{for each possibile class $m$ for the nth 
event which in truth belongs to class $t_{n}$ defined as}
\begin{eqnarray}
  \label{eq:indicator}
  \tau_{n, m} &\equiv& \begin{cases}
  0 & t_{n} \neq m\\
  1 & t_{n} = m
  \end{cases}.
\end{eqnarray}

However, the metric for \plasticc\ must also be compatible with deterministic class assignments, as many classifiers, including some of the most prevalent classifiers in the time-domain astronomy field, can only provide deterministic classifications (probabilities of 0 or 1) and/or binary classifications (``yes'' vs. ``no'' for a particular class without addressing others), the metric must be well-defined for those cases as well as for nontrivial probabilities and multiple classes.

% We calculate the metric within each class $m$ by taking an average of its value $-\ln[p_{n}(m | d_{n})]$ for each true member $n$ of the class.
% Then we weight the metrics for each class by an arbitrary weight $w_{m}$ and take a weighted average of the per-class metrics to produce a global scalar metric.

\subsubsection{Log-loss}
\label{sec:logloss}

The log-loss, also known as the cross-entropy, originates in the field of information theory.
In that context, entropy
\begin{eqnarray}
  \label{eq:entropy}
  H_{n} &=& -\sum_{m=1}^{M}p(m \mid d_{n})\ln[p(m \mid d_{n})]
\end{eqnarray}
\aim{[Rahul: uncertainty of what? you could define it in terms of the number of states or number of messages like it is used in information theory]}
is a measure of uncertainty, independent of the true value of \changes{$t_{n}$}.
Any deterministic classification, regardless of accuracy, minimizes the entropy, and the uncertain classifier of Section~\ref{sec:uncertaindata} provably maximizes the entropy (Murphy12).
The cross-entropy
\aim{[Rahul: Alex, I changed Eqn 6. please check. I think this follows from the cross entropy being the entropy + KL div, and also make sense in terms of limits, for eg. an ideal classifier still gets this to be 0.]}
\changes{
\begin{eqnarray}
  \label{eq:logloss}
  L_{n} &=& -\sum_{m=1}^{M}(\tau_{n, m})\ln[p(m \mid d_{n})]
\end{eqnarray}
}
is the uncertainty of using $p(m \mid d_{n})$ in place of $\tau_{n, m}$.
\aim{[Rahul: Would not the stuff in the parenthesis below be clearer than this]}
(A difference between $L_{n}$ and $H_{n}$ evaluated at $\tau_{n}$ would be the information lost in using $p(m \mid d_{n})$ in place of $\tau_{n, m}$, also known as the Kullback-Leibler Divergence.
See \cite{2018AJ....156...35M} for a comprehensive exploration of the KLD for a continuous 1-dimensional probability space.)

The log-loss has only recently gained some presence in the astronomy literature \citep{hon_deep_2017, hon_deep_2018}. Its greatest strength is that it is straightforwardly interpretable, enabling the metric itself to directly contribute to uncertainty propagation in an inference problem using the probability densities provided by the classifier.
\aim{[Rahul: Are you saying you could rewrite a BEAMs like model in terms of the logloss metric rather than the Probabilities P(m|d)? ]}
\subsubsection{Brier score}
\label{sec:brier}

The Brier score \cite{brier_verification_1950}, given as
\begin{eqnarray}
  \label{eq:brier}
B_{n} &=& \sum_{m=1}^{M}(\tau_{n, m}-p(m \mid d_{n}))^{2}.
\end{eqnarray}
is a mean square error calculated between the true class indicator vector and the estimated probability vector.
It has been used extensively in solar flare forecasting \cite{crown_validation, mays_ensemble_2015, florios_forecasting_2018}, stellar variability identification \citep{richards_construction_2012, armstrong_k2_2016}, and star-galaxy separation \citep{kim_hybrid_2015}.

The Brier score is an attractive option because it both rewards classifiers assigning high probability to the true class and penalizes classifiers for assigning high probability to classes other than the true class.
We expect expect this difference to significantly distinguish the log-loss from the Brier score.

However, its interpretation is less obvious, as its dimensions depend on those of the probability space upon which the posterior estimates are defined. Furthermore, weighting it involves making a choice about whether weights should only be on the \textit{per-object} values $B_{n}$ or whether weights should also enter the individual terms in the metric above. We leave a full investigation of the weighting on the Brier metric for future work.


%\textbf{RH: this isnt super clear, we should discuss}
%\aim{I'm not sure if there's time to actually investigate the latter, nor how forthcoming to be about that being a nontrivial concern that drove the final decision about a metric for Kaggle.}

% \subsubsection{Conditional density estimation loss}
% \label{sec:cdeloss}
%
% \aim{The CDE Loss should also be included in the paper (and sent to Kaggle, if there's still time) if I can figure out how to make it work for non-ordered domains, i.e. classes.}

\subsection{Weights}
\label{sec:weights}

Classifiers can be subject to several types of systematics, introduced in Section~\ref{sec:mockdata}, that can be either mild or severe, and the metrics we choose should reward or penalize a classifier on the basis of its systematics.
The two most concerning systematics are the ``tunnel vision'' and ``cruise control'' classifiers.
% For example, there can be ``tunnel vision'' classifiers that perform very well at identifying one particluar class of objects, while doing a poor job for all the other classes.
% Another example is a ``cruise control'' classifier, that overwhelmingly predicts that all objects are of one particular class.
The most universal underlying causes of these systematics are the imbalance of class membership rates, which may differ by orders of magnitude for the \plasticc\ and \lsst\ datasets, and the nonrepresentative training set.
Both of these problems are anticipated for \lsst\ and thus featured in \plasticc; class prevalence will differ by orders of magnitude, and there will be classes that are not present in the training set at all.
% This can be problematic because our science needs might require accurate classifications of uncommon classes as well.
To derive a final scalar metric, we use a weighted average
\begin{eqnarray}
Q_{tot} &=& \frac{1}{M}\sum_{m=1}^{M}w_{m}Q_{m}
\end{eqnarray}
of the per-class metrics
\begin{eqnarray}
Q_{m} &=& \frac{1}{N_{M}}\sum_{i\in\mathcal{S}_{m}}Q_{i}.
\end{eqnarray}
rather than giving equal weight to each object in the test set.

The weights for the \plasticc\ metric must be determined before the knowledge of which systematics affect which classes exists.
Because of this, the choice of weights is an inherently human problem dictated by the value placed on the scientific merits of knowledge of each class.
This paper, on the other hand, can only quantify the impact of weights in relation to the systematics.
We consider four weighting schemes: equal weight per object (population weighting), equal weight per class (flat weight vector), upweighting an affected class (relative to flat), and downweighting an affected class (relative to flat).

% \subsection{Evaluation of performance of the metric}
% \label{sec:inception}
%
% To identify a metric for \plasticc, it is necessary to define a metric over possible metrics.
% We acknowledge that, as with the weights, the choice of metric is in many ways a human problem.
%  % and defer back to the numbered questions of Section~\ref{sec:intro}.
% We perform qualitative tests of each our our proposed metrics, comparing them to one another and observing how they respond to the controlled introduction of systematics we anticipate will affect \plasticc\ submissions, which are described in the following sections.
