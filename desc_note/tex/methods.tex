\section{Methods}
\label{sec:methods}

%\begin{itemize}
%\item    The metric must return a single scalar value.
%\item    The metric must be well-defined for non-binary classes.
%\item    The metric must balance diverse science use cases in the presence of heavily nonuniform class prevalence.
%\item    The metric must respect the information content of probabilistic classifications.
%\item    The metric must be able to evaluate deterministic classifications.
%\item    The metric must be interpretable, meaning it gives a more optimal value for "good" mock classifiers and a less optimal value for mock classifiers plagued by anticipated systematic errors; in other words, it must pass basic tests of intuition.
%\item    The metric must be reliable, giving consistent results for different instantiations of the same test case.
%\end{itemize}

The metric chosen for a Kaggle challenge is required to return a single scalar value.
Because many classifiers, including some of the most prevalent classifiers in the time-domain astronomy field, can only provide deterministic classifications (probabilities of 0 or 1) and/or binary classifications (``yes'' vs. ``no'' for a particular class), the metric must be well-defined for those cases as well as for nontrivial probabilities and multiple classes.

In Section~\ref{sec:past}, we review the metrics that have been used in previous challenges with deterministic classifications.
In Section~\ref{sec:metrics}, we introduce metrics appropriate for probabilistic classifications.
We take weighted averages of the per-object metrics with per-class weights described in Section~\ref{sec:weights}.
Section~\ref{sec:inception} discusses the systematic way in which we evaluate the performance of the metrics.

\subsection{Traditional metrics}
\label{sec:past}

Classification problems in astronomy have historically focused on separating a heterogenous population into a limited number of subclasses (cite Kessler+2010), with the focus or goal being to identify one particular type of object.

In the Supernova Photometric Classification Challenge (SNPhotCC), the metric for deciding on who `won' the challenge was determined as a ratio of the efficiency of Type Ia classification and a `pseudo purity' factor, and a penalty for false-flagging (which is related to the cost of following up an object spectroscopically that is not a SNIa).
The figure of metric was given by:

\begin{eqnarray}
\mathcal{C}_{FOM-Ia} &\equiv& \frac{1}{\mathcal{N}_{Ia}^{TOT}}\times \frac{(N_{Ia}^{\mathrm{true}})^2}{N_{Ia}^\mathrm{true}+W_{Ia}^\mathrm{false}N_{Ia}^\mathrm{false}}
\end{eqnarray}
%\aim{Renee Hlozek will write the descriptions of these datasets.}

The above reduces to $\mathcal{C}_{FOM-Ia}  = \epsilon_{Ia} + PP_{Ia},$ the efficiency and pseudopurity, which can be interpreted as the traditional purity factor in the limit that the weight $W_{Ia}^\mathrm{false} = 1$.
For the SNPhotCC the false penalty was related to the size of the spectroscopic subsample as roughly $W_{Ia}^\mathrm{false} = 1 + \epsilon_{spec}^{-1} \gg 1$ but the conservative limit of $W_{Ia}^\mathrm{false} = 3$ was chosen to penalize wasted spectroscopic time over rejected SNe.

For future challenges, a more balanced metric can be used to ensure correct classifications across the range of objects, without focusing or highlighting a specific object as above.

\subsection{Probabilistic metrics}
\label{sec:metrics}

\plasticc\ aims to identify classifiers that produce discrete posterior probability distributions $p(m \mid d)$ over $M$ classes $m$ given their photometric lightcurve data $d$.
Such posteriors are more valuable than point estimates, which we call deterministic metrics in this work, because of their versatility in application and encapsulation of observational and systematic error that may propagate through inference.
Traditional classification metrics may be modified for evaluation on probabilistic classifications (Lochner+16, M\"{o}ller+16, Hon,Stello,Zinn18) but only by reducing class probabilities to point estimates of class, so we consider different metrics here.

\aim{Should I add a subsection on traditional metrics used in previous classification challenges in astronomy before introducing the probabilistic metrics we looked at?}

\aim{Alex Malz will describe the probabilistic metrics considered.  Finding actual references is a top priority!}

We consider two (three?) metrics of classification probabilities, each of which is interpretable and avoids reducing probabilities to point estimates.
We define the indicator variable $t_{n, m}$ as
\begin{eqnarray}
  \tau_{n, m} &=& \begin{cases}
  0 & t_{n} \neq m\\
  1 & t_{n} = m
  \end{cases}.
\end{eqnarray}

% We calculate the metric within each class $m$ by taking an average of its value $-\ln[p_{n}(m | d_{n})]$ for each true member $n$ of the class.
% Then we weight the metrics for each class by an arbitrary weight $w_{m}$ and take a weighted average of the per-class metrics to produce a global scalar metric.

\subsubsection{Brier score}
\label{sec:brier}

The Brier score (Brier50) is a mean square error calculated between the true has been used extensively in solar flare forecasting (Crown12, Mays+15, Florios+18), stellar variability identification (Richards+12, Armstrong+15), and star-galaxy separation (Kim+15).

The Brier score for a single object is defined as
\begin{eqnarray}
B_{n} &=& \sum_{m=1}^{M}(\tau_{n, m}-p(m \mid d_{n}))^{2}.
\end{eqnarray}

\subsubsection{Log-loss}
\label{sec:logloss}

\aim{Relate to cross-entropy}

The log-loss, also known as the cross-entropy, is a metric of the information\dots that has only recently gained some presence in the astronomy literature (Hon+17, Hon+18).

The log-loss for a single object is defined as
\begin{eqnarray}
L &=& -\sum_{m=1}^{M}(1-\tau_{n, m})\ln[p_{n}(m | d_{n})]
\end{eqnarray}

Unlike the Brier score, the log-loss does not have an explicit penalty term, a difference we expect expect to significantly distinguish the log-loss from the Brier score.

% \subsubsection{Conditional density estimation loss}
% \label{sec:cdeloss}
%
% \aim{The CDE Loss should also be included in the paper (and sent to Kaggle, if there's still time).}

\subsection{Weights (Rafael Martinez-Galarza)}
\label{sec:weights}

% \aim{Rafael Martinez-Galarza will write the motivation for and definition of the weights.}

Weights are desirable when class membership rates differ by orders of magnitude, as they are the most obvious path toward mitigating the most obvious systematic, that of the most dominant class determining the performance when our science needs require accurate classifications of uncommon classes as well.
We take weighted averages $Q_{tot} = \frac{1}{M}\sum_{m=1}^{M}w_{m}Q_{m}$ of the per-class metrics $Q_{m} = \frac{1}{N_{m}}\sum_{i\in\mathcal{S}_{m}}Q_{i}$, and these weights $w_{m}$ may be considered in terms of the systematics we discussed, by upweighting or downweighting the "chosen" class most affected by the systematics.

\subsection{Evaluation of performance of the metric}
\label{sec:inception}

To identify a metric for \plasticc, it is necessary to define a metric over possible metrics.
We acknowledge that, as with the weights, the choice of metric is in many ways a human problem.
 % and defer back to the numbered questions of Section~\ref{sec:intro}.
We perform qualitative tests of each our our proposed metrics, comparing them to one another and observing how they respond to the controlled introduction of systematics we anticipate will affect \plasticc\ submissions, which are described in the following sections.
