\section{Methods}
\label{sec:methods}

%\begin{itemize}
%\item    The metric must return a single scalar value.
%\item    The metric must be well-defined for non-binary classes.
%\item    The metric must balance diverse science use cases in the presence of heavily nonuniform class prevalence.
%\item    The metric must respect the information content of probabilistic classifications.
%\item    The metric must be able to evaluate deterministic classifications.
%\item    The metric must be interpretable, meaning it gives a more optimal value for "good" mock classifiers and a less optimal value for mock classifiers plagued by anticipated systematic errors; in other words, it must pass basic tests of intuition.
%\item    The metric must be reliable, giving consistent results for different instantiations of the same test case.
%\end{itemize}

The metric chosen for a Kaggle challenge is required to return a single scalar value.
Because many classifiers, including some of the most prevalent classifiers in the time-domain astronomy field, can only provide deterministic classifications (probabilities of 0 or 1) and/or binary classifications (``yes'' vs. ``no'' for a particular class), the metric must be well-defined for those cases as well as for nontrivial probabilities and multiple classes.

In Section~\ref{sec:metrics}, we introduce several metrics.
We take weighted averages of the per-object metrics with per-class weights described in Section~\ref{sec:weights}.
Section~\ref{sec:inception} discusses the systematic way in which we evaluate the performance of the metrics.

\subsection{Probabilistic metrics}
\label{sec:metrics}

\plasticc\ aims to identify classifiers that produce discrete posterior probability distributions $p(m \mid d)$ over $M$ classes $m$ given their photometric lightcurve data $d$.
Such posteriors are more valuable than point estimates, which we call deterministic metrics in this work, because of their versatility in application and encapsulation of observational and systematic error that may propagate through inference.
Traditional classification metrics may be modified for evaluation on probabilistic classifications (Lochner+16, M\"{o}ller+16, Hon,Stello,Zinn18) but only by reducing class probabilities to point estimates of class, so we consider different metrics here.

\aim{Should I add a subsection on traditional metrics used in previous classification challenges in astronomy before introducing the probabilistic metrics we looked at?}

\aim{Alex Malz will describe the probabilistic metrics considered.  Finding actual references is a top priority!}

We consider two (three?) metrics of classification probabilities, each of which is interpretable and avoids reducing probabilities to point estimates.
We define the indicator variable $t_{n, m}$ as
\begin{eqnarray}
  \tau_{n, m} &=& \begin{cases}
  0 & t_{n} \neq m\\
  1 & t_{n} = m
  \end{cases}.
\end{eqnarray}

% We calculate the metric within each class $m$ by taking an average of its value $-\ln[p_{n}(m | d_{n})]$ for each true member $n$ of the class.
% Then we weight the metrics for each class by an arbitrary weight $w_{m}$ and take a weighted average of the per-class metrics to produce a global scalar metric.

\subsubsection{Brier score}
\label{sec:brier}

The Brier score (Brier50) is a mean square error calculated between the true has been used extensively in solar flare forecasting (Crown12, Mays+15, Florios+18), stellar variability identification (Richards+12, Armstrong+15), and star-galaxy separation (Kim+15).

The Brier score for a single object is defined as
\begin{eqnarray}
B_{n} &=& \sum_{m=1}^{M}(\tau_{n, m}-p(m \mid d_{n}))^{2}.
\end{eqnarray}

\subsubsection{Log-loss}
\label{sec:logloss}

\aim{Relate to cross-entropy}

The log-loss, also known as the cross-entropy, is a metric of the information\dots that has only recently gained some presence in the astronomy literature (Hon+17, Hon+18).

The log-loss for a single object is defined as
\begin{eqnarray}
L &=& -\sum_{m=1}^{M}(1-\tau_{n, m})\ln[p_{n}(m | d_{n})]
\end{eqnarray}

Unlike the Brier score, the log-loss does not have an explicit penalty term, a difference we expect expect to significantly distinguish the log-loss from the Brier score.

% \subsubsection{Conditional density estimation loss}
% \label{sec:cdeloss}
%
% \aim{The CDE Loss should also be included in the paper (and sent to Kaggle, if there's still time).}

\subsection{Weights (Rafael Martinez-Galarza)}
\label{sec:weights}

% \aim{Rafael Martinez-Galarza will write the motivation for and definition of the weights.}
Classifiers can be subject to several types of systematics that can be either mild or sever, and the metrics we choose should be able to evaluate, and where appropriate panalize those systematics. For example, there can be ``tunnel vision'' classifiers that perform very well at identifying one particluar class of objects, while doing a poor job for all the other classes. Another example is a ``cruise control'' classifier, that overwhelmingly predicts that all objects are of one particular class. In the context of the challenge, these systematics mostly arise because of the class membership rates differ by orders of magnitude, and therefore some of those classes are underrepresented in the training set, leading to classification underperformance. This can be problematic because our science needs might require accurate classifications of uncommon classes as well.

One way to deal with these systematics is by considering weights associated to the different classes. In general terms, the idea is to take weighted averages of the per-class metric $Q_{m} = (1/N_{m})\sum_{i\in\mathcal{S}_{m}}Q_{i}$:

\begin{equation}
$Q_{tot} = \frac{1}{M}\sum_{m=1}^{M}w_{m}Q_{m}$
\end{equation}

These weights $w_{m}$ may be considered in terms of the systematics we discussed, by upweighting or downweighting the "chosen" class most affected by the systematics.

In order to study the sensitivity of our metrics to systematics, we tested different weighting schemes with different types of systematics. In particular, we tested the following weighting schemes: \emph{1)} equal weight per object; \emph{2)} equal weight per class; \emph{3)} upweighting a class affected by a systematic; and \emph{4)} downweighting a class affected by a systematic.



\subsection{Evaluation of performance of the metric}
\label{sec:inception}

To identify a metric for \plasticc, it is necessary to define a metric over possible metrics.
We acknowledge that, as with the weights, the choice of metric is in many ways a human problem.
 % and defer back to the numbered questions of Section~\ref{sec:intro}.
We perform qualitative tests of each our our proposed metrics, comparing them to one another and observing how they respond to the controlled introduction of systematics we anticipate will affect \plasticc\ submissions, which are described in the following sections.
