{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disagreement metrics\n",
    "\n",
    "_Alex Malz (GCCL@RUB)_ and _Mi Dai_, _Kara Ponder_ {add your names here!}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "epsilon = sys.float_info.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('PS')\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "mpl.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "mpl.rcParams['font.serif'] = 'DejaVu Serif'\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['savefig.dpi'] = 250\n",
    "mpl.rcParams['figure.dpi'] = 250\n",
    "mpl.rcParams['savefig.format'] = 'pdf'\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import pylab\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {90: 'SNIa',\n",
    "              67: 'SNIa-91bg',\n",
    "              52: 'SNIax',\n",
    "              42: 'SNII',\n",
    "              62: 'SNIbc',\n",
    "              95: 'SLSN-I',\n",
    "              15: 'TDE',\n",
    "              64: 'KN',\n",
    "              88: 'AGN',\n",
    "              92: 'RRL',\n",
    "              65: 'M-dwarf',\n",
    "              16: 'EB',\n",
    "              53: 'Mira',\n",
    "              6: r'$\\mu$Lens-Single'}\n",
    "\n",
    "true_labels = label_dict.copy()\n",
    "true_labels[991] = r'$\\mu$Lens-Binary'\n",
    "true_labels[992] = 'ILOT'\n",
    "true_labels[993] = 'CaRT'\n",
    "true_labels[994] = 'PISN'\n",
    "true_labels[995] = r'$mu$Lens-String'\n",
    "\n",
    "sub_labels = label_dict.copy()\n",
    "sub_labels[99] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contestants = ['1_Kyle', '2_MikeSilogram', '3_MajorTom', '4_AhmetErdem','5_SKZLostInTranslation','6_StefanStefanov',\n",
    "#                '7_hklee',\n",
    "               '8_rapidsai','9_ThreeMusketeers', \n",
    "#                '10_JJ',\n",
    "               '11_SimonChen', '12_Go_Spartans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 50\n",
    "positions = np.linspace(0., 1., nbins)\n",
    "dx = 1. / nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_check_violin(one_target, cl_est, contestant, vb=False):\n",
    "    thepath = os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')\n",
    "    data = np.genfromtxt(thepath)\n",
    "    if np.any(data < 0.):\n",
    "        print(data[data < 0.])\n",
    "        data[data < epsilon] = epsilon\n",
    "    if np.all(np.isclose(data, 0.)):\n",
    "        data[0] = 1. / dx\n",
    "    infcheck = np.isinf(data)\n",
    "    nancheck = np.isnan(data)\n",
    "    if np.any(infcheck):\n",
    "        data[infcheck] = 1. / sum(infcheck) / dx\n",
    "        data[~infcheck] = 0.\n",
    "    elif np.any(nancheck):\n",
    "        data[nancheck] = 1. / sum(nancheck) / dx\n",
    "        data[~nancheck] = 0.\n",
    "    norm = np.sum(data * dx)\n",
    "    if not np.isclose(norm, 1.):\n",
    "        data = data / norm\n",
    "    assert(np.isclose(1., np.sum(data * dx)))\n",
    "    if vb:\n",
    "        print(\"I just read in \"+contestant+\"'s violin curve of true \"+label_dict[one_target]+\" assigned \"+sub_labels[cl_est])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for one_target in label_dict.keys():\n",
    "    for cl_est in label_dict.keys():\n",
    "        for contestant in contestants:\n",
    "            data = read_and_check_violin(one_target, cl_est, contestant, vb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar metrics of disparity between 1-D probability densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler divergence\n",
    "\n",
    "Of each classifier relative to a \"ground truth\" of `'1_Kyle'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original qp implementation\n",
    "def safelog(arr, threshold=epsilon):\n",
    "    \"\"\"\n",
    "    Takes the natural logarithm of an array of potentially non-positive numbers\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: numpy.ndarray, float\n",
    "        values to be logged\n",
    "    threshold: float\n",
    "        small, positive value to replace zeros and negative numbers\n",
    "    Returns\n",
    "    -------\n",
    "    logged: numpy.ndarray\n",
    "        logarithms, with approximation in place of zeros and negative numbers\n",
    "    \"\"\"\n",
    "    shape = np.shape(arr)\n",
    "    flat = arr.flatten()\n",
    "    logged = np.log(np.array([max(a, threshold) for a in flat])).reshape(shape)\n",
    "    return logged\n",
    "\n",
    "def quick_kld(p_eval, q_eval, dx=0.01):\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler Divergence between two evaluations of PDFs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_eval: numpy.ndarray, float\n",
    "        evaluations of probability distribution whose distance _from_ `q` will be calculated\n",
    "    q_eval: numpy.ndarray, float\n",
    "        evaluations of probability distribution whose distance _to_ `p` will be calculated.\n",
    "    dx: float\n",
    "        resolution of integration grid\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dpq: float\n",
    "        the value of the Kullback-Leibler Divergence from `q` to `p`\n",
    "    \"\"\"\n",
    "    logquotient = safelog(p_eval) - safelog(q_eval)\n",
    "    # logp = safelog(pn)\n",
    "    # logq = safelog(qn)\n",
    "    # Calculate the KLD from q to p\n",
    "    Dpq = dx * np.sum(p_eval * logquotient)\n",
    "    return Dpq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot one example: class vs. class per classifier KLD relative to Kyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlab = len(label_dict.keys())\n",
    "# all_klds = {}\n",
    "# for k, contestant in enumerate(contestants[1:]):\n",
    "#     all_klds[contestant] = np.empty((nlab, nlab))\n",
    "# labmatrix = []\n",
    "# for i, one_target in enumerate(label_dict.keys()):\n",
    "#     labmatrix.append([])\n",
    "#     for j, cl_est in enumerate(label_dict.keys()):\n",
    "#         labmatrix[i].append((one_target, cl_est))\n",
    "#         p_eval = read_and_check_violin(one_target, cl_est, contestants[0], vb=False)\n",
    "#         for k, contestant in enumerate(contestants[1:]):\n",
    "#             q_eval = read_and_check_violin(one_target, cl_est, contestant, vb=False)\n",
    "#             kld = quick_kld(p_eval, q_eval, dx=dx)\n",
    "#             all_klds[contestant][i][j] = kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlab = len(label_dict.keys())\n",
    "moar_klds = {}\n",
    "for k, contestant_true in enumerate(contestants):\n",
    "    moar_klds[contestant_true+'_as_true'] = {}\n",
    "    for l, contestant_approx in enumerate(contestants):\n",
    "        moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx'] = np.empty((nlab, nlab))\n",
    "\n",
    "labmatrix = []\n",
    "for i, one_target in enumerate(label_dict.keys()):\n",
    "    labmatrix.append([])\n",
    "    for j, cl_est in enumerate(label_dict.keys()):\n",
    "        labmatrix[i].append((one_target, cl_est))\n",
    "        for k, contestant_true in enumerate(contestants):\n",
    "            p_eval = read_and_check_violin(one_target, cl_est, contestant_true, vb=False)\n",
    "            for l, contestant_approx in enumerate(contestants):\n",
    "                q_eval = read_and_check_violin(one_target, cl_est, contestant_approx, vb=False)\n",
    "                kld = quick_kld(p_eval, q_eval, dx=dx)\n",
    "                if kld < 0.:\n",
    "#                     print(kld)\n",
    "                    kld = epsilon\n",
    "                moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx'][i][j] = kld\n",
    "                if l == k:\n",
    "                    assert(np.isclose(kld, 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_ex = contestants[7]\n",
    "# contestant_true = contestants[7]\n",
    "# contestant_approx = contestants[9]\n",
    "\n",
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = plt.gca()\n",
    "# plt.title('KLD of %s relative to %s'%(contestant_true, contestant_approx))\n",
    "# m = ax.imshow(moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx'], interpolation='nearest', cmap=plt.cm.summer, \n",
    "#               vmin=0., vmax=np.amax(moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx']))\n",
    "# tick_marks = np.arange(nlab)\n",
    "# ax.set_xticks(tick_marks)\n",
    "# ax.set_xticklabels(label_dict.values(), rotation=45, size=12)\n",
    "# ax.set_yticks(tick_marks)\n",
    "# ax.set_yticklabels(label_dict.values(), size=12)\n",
    "\n",
    "# fig.colorbar(m, ax=ax)\n",
    "\n",
    "# for k, l in itertools.product(range(nlab), range(nlab)):\n",
    "# #     if k == l:\n",
    "# #         continue\n",
    "# #     else:\n",
    "#     thresh = np.amax(moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx']) / 2.\n",
    "#     ax.text(l, k, format(moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx'][k, l], '.2f') ,\n",
    "#                 horizontalalignment=\"center\", size=14,\n",
    "#                color=\"black\")# if num[k, l] > thresh else \"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot one example, classifier vs. classifier KLD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwin = len(contestants)\n",
    "summarized_klds = np.zeros((nwin, nwin))\n",
    "nam = np.chararray((nwin, nwin), itemsize=30)\n",
    "percent_kld = np.zeros((nwin, nwin))\n",
    "\n",
    "for k, contestant_true in enumerate(contestants):\n",
    "    for l, contestant_approx in enumerate(contestants):\n",
    "        if l >= k:\n",
    "            summarized_klds[k][l] = np.sum(moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx'])\n",
    "            vsum = np.sum(moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx'], axis=1)\n",
    "            nam[k, l] = list(label_dict.values())[np.argmax(vsum)]\n",
    "            percent_kld[k, l] = max(vsum)/sum(vsum)\n",
    "            #print(max(vsum), summarized_klds[k][l])\n",
    "            if l == k and ~np.isclose(summarized_klds[k][l], 0.):\n",
    "                print((contestant_true+'_as_true', contestant_approx+'_as_approx'))\n",
    "                print(np.max(moar_klds[contestant_true+'_as_true'][contestant_approx+'_as_approx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = plt.gca()\n",
    "#plt.title('integrated KLDs')\n",
    "\n",
    "colMap = plt.cm.inferno\n",
    "colMap.set_under(color='white')\n",
    "\n",
    "m = ax.imshow(summarized_klds[:-1,1:], interpolation='nearest', cmap=colMap, \n",
    "              norm=LogNorm(vmin=np.amin(summarized_klds[np.where(summarized_klds>0)]), \n",
    "                           vmax=np.amax(summarized_klds)))\n",
    "\n",
    "tick_marks = np.arange(nwin-1)\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_xticklabels(contestants[1:], rotation=45, size=14, ha='right')\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_yticklabels(contestants[:-1], size=14)\n",
    "ax.set_ylabel(\"As True Classifier\", rotation=270, labelpad=15)\n",
    "ax.set_xlabel(\"As Approximating Classifier\")\n",
    "\n",
    "ax.yaxis.tick_right()\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"left\", size=\"5%\", pad=0.05)\n",
    "cbar = fig.colorbar(m, cax=cax, ticks=[7, 10, 15, 20, 30, 40, 55, 70, 90, 110])\n",
    "cax.yaxis.set_ticks_position('left')\n",
    "cbar.set_label(label='Scalarized KLD', fontsize=20, labelpad=10)\n",
    "cbar.ax.set_yticklabels([7, 10, 15, 20, 30, 40, 55, 70, 90, 110], fontsize=14)\n",
    "\n",
    "sk = summarized_klds[:-1,1:]\n",
    "nsk = nam[:-1,1:]\n",
    "pk = percent_kld[:-1,1:]\n",
    "\n",
    "for k, l in itertools.product(range(nwin-1), range(nwin-1)):\n",
    "    thresh = np.amax(sk) / 3.\n",
    "    if sk[k, l] > 0:\n",
    "        print((k, l, sk[k, l] > thresh))\n",
    "        ax.text(l, k+0.1, nsk[k,l].decode(\"utf-8\")+'\\n'+format(pk[k, l]*100, '.0f')+'%' ,\n",
    "                    horizontalalignment=\"center\", size=15,\n",
    "                   color=\"black\" if sk[k, l] > thresh else \"white\")\n",
    "\n",
    "    \n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "# plt.savefig('kld_inferno_labels_percent_11.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE\n",
    "\n",
    "Between classifier pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import (mean_squared_error, r2_score, max_error,\n",
    "#                             mean_squared_log_error, median_absolute_error)\n",
    "#                              #mean_poisson_deviance, mean_gamma_deviance, mean_tweedie_deviance\n",
    "\n",
    "# func = [mean_squared_error, r2_score, max_error, mean_squared_log_error, median_absolute_error]\n",
    "\n",
    "# dm = np.zeros((len(label_dict), len(label_dict), len(contestants)-1, len(func)))\n",
    "\n",
    "# for i, one_target in enumerate(label_dict.keys()):\n",
    "#     for j, cl_est in enumerate(label_dict.keys()):\n",
    "#         kthepath = os.path.join('submissions/1_Kyle', 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')\n",
    "#         kyle = np.genfromtxt(kthepath)\n",
    "#         if np.all(kyle == 0.):\n",
    "#             kyle[0] = 1. / dx\n",
    "#         infcheck = np.isinf(kyle)\n",
    "#         nancheck = np.isnan(kyle)\n",
    "#         if np.any(infcheck):\n",
    "#             kyle[infcheck] = 1. / sum(infcheck) / dx\n",
    "#             kyle[~infcheck] = 0.\n",
    "#         elif np.any(nancheck):\n",
    "#             kyle[nancheck] = 1. / sum(nancheck) / dx\n",
    "#             kyle[~nancheck] = 0.\n",
    "#         norm = np.sum(kyle * dx)\n",
    "#         if not np.isclose(norm, 1.):\n",
    "#             kyle = kyle / norm\n",
    "#         assert(np.isclose(1., np.sum(kyle * dx)))\n",
    "\n",
    "#         for k, contestant in enumerate(contestants[1:]):\n",
    "            \n",
    "#             thepath = os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')\n",
    "#             data = np.genfromtxt(thepath)\n",
    "#             if np.all(data == 0.):\n",
    "#                 data[0] = 1. / dx\n",
    "#             infcheck = np.isinf(data)\n",
    "#             nancheck = np.isnan(data)\n",
    "#             if np.any(infcheck):\n",
    "#                 data[infcheck] = 1. / sum(infcheck) / dx\n",
    "#                 data[~infcheck] = 0.\n",
    "#             elif np.any(nancheck):\n",
    "#                 data[nancheck] = 1. / sum(nancheck) / dx\n",
    "#                 data[~nancheck] = 0.\n",
    "#             norm = np.sum(data * dx)\n",
    "#             if not np.isclose(norm, 1.):\n",
    "#                 data = data / norm\n",
    "#             assert(np.isclose(1., np.sum(data * dx)))\n",
    "                \n",
    "#             #print(\"Compare \"+contestant+\"'s true \"+label_dict[one_target]+\" assigned \"+sub_labels[cl_est] + ' to Kyle')\n",
    "#             #print(mean_squared_error(kyle, data), mean_squared_log_error(kyle, data), \n",
    "#             #      r2_score(kyle, data), max_error(kyle, data), median_absolute_error(kyle, data))\n",
    "#             #print('kyle', kyle, '\\ndata: ', data)\n",
    "#             for l, f in enumerate(func):\n",
    "#                 dm[i, j, k, l] = f(kyle, data)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein distance\n",
    "\n",
    "Between classifier pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "# import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_dict = {}\n",
    "# for one_target in label_dict.keys():\n",
    "#     data_dict[one_target] = {}\n",
    "#     for cl_est in label_dict.keys():\n",
    "#         data_dict[one_target][cl_est] = {}\n",
    "#         for contestant in contestants:\n",
    "#             thepath = os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')\n",
    "#             data = np.genfromtxt(thepath)\n",
    "#             if np.all(data == 0.):\n",
    "#                 data[0] = 1. / dx\n",
    "#             infcheck = np.isinf(data)\n",
    "#             nancheck = np.isnan(data)\n",
    "#             if np.any(infcheck):\n",
    "#                 data[infcheck] = 1. / sum(infcheck) / dx\n",
    "#                 data[~infcheck] = 0.\n",
    "#             elif np.any(nancheck):\n",
    "#                 data[nancheck] = 1. / sum(nancheck) / dx\n",
    "#                 data[~nancheck] = 0.\n",
    "#             norm = np.sum(data * dx)\n",
    "#             if not np.isclose(norm, 1.):\n",
    "#                 data = data / norm\n",
    "#             assert(np.isclose(1., np.sum(data * dx)))\n",
    "# #             print(\"I just read in \"+contestant+\"'s violin curve of true \"+label_dict[one_target]+\" assigned \"+sub_labels[cl_est])\n",
    "#             data_dict[one_target][cl_est][contestant] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_pairs = list(itertools.combinations(list(label_dict.keys()),2))\n",
    "# contestant_pairs = list(itertools.combinations(list(contestants),2))\n",
    "\n",
    "# # print(list(target_pairs))\n",
    "# # print(list(contestant_pairs))\n",
    "\n",
    "# res_dict = {}\n",
    "# for cp in list(contestant_pairs):\n",
    "#     res_dict[cp] = {}\n",
    "#     for tp in list(target_pairs):\n",
    "# #         print(cp,tp)\n",
    "#         wd = wasserstein_distance(data_dict[tp[0]][tp[1]][cp[0]],data_dict[tp[0]][tp[1]][cp[1]])\n",
    "#         res_dict[cp][tp] = wd\n",
    "# for cp in list(contestant_pairs):\n",
    "#     for tp in list(label_dict.keys()):\n",
    "# #         print(cp,tp)\n",
    "#         wd = wasserstein_distance(data_dict[tp][tp][cp[0]],data_dict[tp][tp][cp[1]])\n",
    "#         res_dict[cp][(tp,tp)] = wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict[90][92]['1_Kyle'],data_dict[90][92]['2_MikeSilogram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disagree_matrices = {}\n",
    "# for cp in list(contestant_pairs):\n",
    "#     matrix = np.ndarray((len(label_dict.keys()),len(label_dict.keys())))\n",
    "#     for i,one_target in enumerate(list(label_dict.keys())):\n",
    "#         for j,cl_est in enumerate(list(label_dict.keys())):\n",
    "#             if (one_target,cl_est) in res_dict[cp].keys():\n",
    "#                 matrix[i][j] = res_dict[cp][(one_target,cl_est)]\n",
    "#             elif (cl_est,one_target) in res_dict[cp].keys():\n",
    "#                 matrix[i][j] = res_dict[cp][(cl_est,one_target)]               \n",
    "#     disagree_matrices[cp] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# import itertools\n",
    "\n",
    "# def make_disagreement(contestant_pair):\n",
    "#     cp = contestant_pair\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = plt.gca()\n",
    "#     plt.title('%s and %s'%(contestant_pair[0],contestant_pair[1]))\n",
    "#     m = ax.imshow(disagree_matrices[cp], interpolation='nearest', cmap=plt.cm.summer,\n",
    "#                        vmin=0., vmax=np.amax(disagree_matrices[cp]))\n",
    "#     tick_marks = np.arange(len(label_dict))\n",
    "#     ax.set_xticks(tick_marks)\n",
    "#     ax.set_xticklabels(label_dict, rotation=45, size=12)\n",
    "#     ax.set_yticks(tick_marks)\n",
    "#     ax.set_yticklabels(label_dict, size=12)\n",
    "\n",
    "#     for k, l in itertools.product(range(disagree_matrices[cp].shape[0]), range(disagree_matrices[cp].shape[1])):\n",
    "#         if i == j:\n",
    "#             thresh = disagree_matrices[cp].max() / 2.\n",
    "#         else:\n",
    "#             thresh = np.amax(disagree_matrices[cp]) / 2.\n",
    "#         ax.text(l, k, format(disagree_matrices[cp][k,l], '.2f'),\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"black\" if disagree_matrices[cp][k, l] > thresh else \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# for cp in list(contestant_pairs[0:]):\n",
    "# #     plt.imshow(disagree_matrices[cp])\n",
    "# #     plt.show()\n",
    "#     make_disagreement(cp)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_matrix(matrix):\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     ax = plt.gca()\n",
    "# #     plt.title('%s and %s'%(contestant_pair[0],contestant_pair[1]))\n",
    "#     m = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.summer,\n",
    "#                        vmin=0., vmax=np.amax(matrix))\n",
    "#     tick_marks = np.arange(np.shape(matrix)[0])\n",
    "#     ax.set_xticks(tick_marks)\n",
    "#     ax.set_xticklabels(contestants, rotation=45, size=12)\n",
    "#     ax.set_yticks(tick_marks)\n",
    "#     ax.set_yticklabels(contestants, size=12)\n",
    "\n",
    "#     for k, l in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "#         if i == j:\n",
    "#             thresh = disagree_matrices[cp].max() / 2.\n",
    "#         else:\n",
    "#             thresh = np.amax(matrix) / 2.\n",
    "#         ax.text(l, k, format(matrix[k,l], '.2f'),\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"black\" if matrix[k, l] > thresh else \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumsq_matrix = np.zeros((10,10))\n",
    "# for cp in list(contestant_pairs[0:]):\n",
    "#     i = int(cp[0].split('_')[0])-1\n",
    "#     j = int(cp[1].split('_')[0])-1\n",
    "# #     print(i,j)\n",
    "#     sumsq_matrix[i,j] = np.sum(disagree_matrices[cp]**2)\n",
    "# plot_matrix(sumsq_matrix)\n",
    "# plt.title(\"Sum square of each disagreement pair\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing metrics\n",
    "\n",
    "Plot the per-true-class, per-assigned-class, per-classifier metrics as a per-classifier $N_{class} \\times N_{class}$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# import itertools\n",
    "\n",
    "# for i, contestant in enumerate(contestants[1:]):\n",
    "#     for j, f in enumerate(func):\n",
    "#         fig = plt.figure(figsize=(10,10))\n",
    "#         ax = plt.gca()\n",
    "#         plt.title('%s %s'%(contestant, str(f)))\n",
    "#         m = ax.imshow(dm[:, :, i, j], interpolation='nearest', cmap=plt.cm.summer,\n",
    "#                            vmin=0., vmax=np.amax(dm[:, :, i, j]))\n",
    "#         tick_marks = np.arange(len(label_dict))\n",
    "#         ax.set_xticks(tick_marks)\n",
    "#         ax.set_xticklabels(label_dict.values(), rotation=45, size=12)\n",
    "#         ax.set_yticks(tick_marks)\n",
    "#         ax.set_yticklabels(label_dict.values(), size=12)\n",
    "        \n",
    "#         for k, l in itertools.product(range(dm[:, :, i, j].shape[0]), range(dm[:, :, i, j].shape[1])):\n",
    "#             if i == j:\n",
    "#                 thresh = dm[:, :, i, j].max() / 2.\n",
    "#             else:\n",
    "#                 thresh = np.amax(dm[:, :, i, j]) / 2.\n",
    "#             ax.text(l, k, format(dm[k, l, i, j], '.2f'),\n",
    "#                      horizontalalignment=\"center\",\n",
    "#                      color=\"black\" if dm[:, :, i, j][k, l] > thresh else \"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing metrics\n",
    "\n",
    "Combine per-true-class, per-assigned-class, per-classifier-pair metrics into one scalar value per pair of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAP: an option for disagreement matrix\n",
    "#### done using RMSE as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = mean_squared_error\n",
    "\n",
    "# dm1 = np.zeros((len(label_dict), len(label_dict), len(contestants), len(contestants)))\n",
    "\n",
    "# for i, one_target in enumerate(label_dict.keys()):\n",
    "#     for j, cl_est in enumerate(label_dict.keys()):\n",
    "#         for k, contestant in enumerate(contestants):\n",
    "#             kthepath = os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')\n",
    "#             kyle = np.genfromtxt(kthepath)\n",
    "#             if np.all(kyle == 0.):\n",
    "#                 kyle[0] = 1. / dx\n",
    "#             infcheck = np.isinf(kyle)\n",
    "#             nancheck = np.isnan(kyle)\n",
    "#             if np.any(infcheck):\n",
    "#                 kyle[infcheck] = 1. / sum(infcheck) / dx\n",
    "#                 kyle[~infcheck] = 0.\n",
    "#             elif np.any(nancheck):\n",
    "#                 kyle[nancheck] = 1. / sum(nancheck) / dx\n",
    "#                 kyle[~nancheck] = 0.\n",
    "#             norm = np.sum(kyle * dx)\n",
    "#             if not np.isclose(norm, 1.):\n",
    "#                 kyle = kyle / norm\n",
    "#             assert(np.isclose(1., np.sum(kyle * dx)))\n",
    "\n",
    "#             for l, contestant1 in enumerate(contestants):\n",
    "#                 #print(contestant, contestant1)\n",
    "#                 thepath = os.path.join('submissions/'+contestant1, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')\n",
    "#                 data = np.genfromtxt(thepath)\n",
    "#                 if np.all(data == 0.):\n",
    "#                     data[0] = 1. / dx\n",
    "#                 infcheck = np.isinf(data)\n",
    "#                 nancheck = np.isnan(data)\n",
    "#                 if np.any(infcheck):\n",
    "#                     data[infcheck] = 1. / sum(infcheck) / dx\n",
    "#                     data[~infcheck] = 0.\n",
    "#                 elif np.any(nancheck):\n",
    "#                     data[nancheck] = 1. / sum(nancheck) / dx\n",
    "#                     data[~nancheck] = 0.\n",
    "#                 norm = np.sum(data * dx)\n",
    "#                 if not np.isclose(norm, 1.):\n",
    "#                     data = data / norm\n",
    "#                 assert(np.isclose(1., np.sum(data * dx)))\n",
    "\n",
    "#                 dm1[i, j, k, l] = metric(kyle, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # axis = 0 sums over prediction\n",
    "# # axis = 1 sums over true\n",
    "\n",
    "# num = np.zeros((len(contestants), len(contestants)))\n",
    "# nam = np.chararray((len(contestants), len(contestants)), itemsize=30)\n",
    "\n",
    "# for i in range(0,len(contestants)):\n",
    "#     for j in range(0,len(contestants)):\n",
    "#         s = np.sum(dm1[:,:, i, j], axis=1)\n",
    "#         a = np.argmax(s)\n",
    "#         num[i,j] = s[a]\n",
    "#         nam[i,j] = list(label_dict.values())[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = plt.gca()\n",
    "# #plt.title('%s %s'%(contestant, str(f)))\n",
    "# m = ax.imshow(num, interpolation='nearest', cmap=plt.cm.summer,\n",
    "#                    vmin=0., vmax=np.amax(num))\n",
    "# tick_marks = np.arange(len(contestants))\n",
    "# ax.set_xticks(tick_marks)\n",
    "# ax.set_xticklabels(contestants, rotation=45, size=12)\n",
    "# ax.set_yticks(tick_marks)\n",
    "# ax.set_yticklabels(contestants, size=12)\n",
    "\n",
    "# fig.colorbar(m, ax=ax)\n",
    "\n",
    "# for k, l in itertools.product(range(num.shape[0]), range(num.shape[1])):\n",
    "#     if k == l:\n",
    "#         continue\n",
    "#     else:\n",
    "#         thresh = np.amax(num) / 2.\n",
    "#         ax.text(l, k, nam[k,l].decode(\"utf-8\")+'\\n'+format(num[k, l], '.2f') ,\n",
    "#                 horizontalalignment=\"center\", size=14,\n",
    "#                color=\"black\" if num[k, l] > thresh else \"white\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum and Average over all values in Contestant v Contestant disagreement metric matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summed = np.zeros((len(contestants), len(contestants)))\n",
    "# averaged = np.zeros((len(contestants), len(contestants)))\n",
    "\n",
    "# for i in range(0,len(contestants)):\n",
    "#     for j in range(0,len(contestants)):\n",
    "#         summed[i, j] = np.sum(dm1[:,:, i, j])\n",
    "#         averaged[i, j] = np.average(dm1[:,:, i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15,15))\n",
    "# ax = plt.gca()\n",
    "# plt.title('Summed disagreement across all classes')\n",
    "# m = ax.imshow(summed, interpolation='nearest', cmap=plt.cm.summer,\n",
    "#                    vmin=0., vmax=np.amax(summed))\n",
    "# tick_marks = np.arange(len(contestants))\n",
    "# ax.set_xticks(tick_marks)\n",
    "# ax.set_xticklabels(contestants, rotation=45, size=12)\n",
    "# ax.set_yticks(tick_marks)\n",
    "# ax.set_yticklabels(contestants, size=12)\n",
    "\n",
    "# fig.colorbar(m, ax=ax)\n",
    "\n",
    "# for k, l in itertools.product(range(summed.shape[0]), range(summed.shape[1])):\n",
    "#     if k == l:\n",
    "#         continue\n",
    "#     else:\n",
    "#         thresh = np.amax(num) / 2.\n",
    "#         ax.text(l, k, format(summed[k, l], '.2f'),\n",
    "#                 horizontalalignment=\"center\", size=14,\n",
    "#                color=\"black\" if num[k, l] > thresh else \"white\")\n",
    "        \n",
    " \n",
    "\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "# ax = plt.gca()\n",
    "# plt.title('Averaged disagreement across all classes')\n",
    "# m = ax.imshow(averaged, interpolation='nearest', cmap=plt.cm.summer,\n",
    "#                    vmin=0., vmax=np.amax(averaged))\n",
    "# tick_marks = np.arange(len(contestants))\n",
    "# ax.set_xticks(tick_marks)\n",
    "# ax.set_xticklabels(contestants, rotation=45, size=12)\n",
    "# ax.set_yticks(tick_marks)\n",
    "# ax.set_yticklabels(contestants, size=12)\n",
    "\n",
    "# fig.colorbar(m, ax=ax)\n",
    "\n",
    "# for k, l in itertools.product(range(averaged.shape[0]), range(averaged.shape[1])):\n",
    "#     if k == l:\n",
    "#         continue\n",
    "#     else:\n",
    "#         thresh = np.amax(num) / 2.\n",
    "#         ax.text(l, k, format(averaged[k, l], '.2f') ,\n",
    "#                 horizontalalignment=\"center\", size=14,\n",
    "#                color=\"black\" if num[k, l] > thresh else \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proclam (Python 3)",
   "language": "python",
   "name": "proclam_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
