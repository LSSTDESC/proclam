{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disagreement metrics\n",
    "\n",
    "_Alex Malz (GCCL@RUB)_ and {add your names here!}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# mpl.use('PS')\n",
    "# mpl.rcParams['text.usetex'] = False\n",
    "# mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "# mpl.rcParams['font.family'] = 'serif'\n",
    "# mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "# mpl.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "# mpl.rcParams['font.serif'] = 'DejaVu Serif'\n",
    "# mpl.rcParams['axes.titlesize'] = 20\n",
    "# mpl.rcParams['axes.labelsize'] = 16\n",
    "# mpl.rcParams['xtick.labelsize'] = 12\n",
    "# mpl.rcParams['ytick.labelsize'] = 12\n",
    "# mpl.rcParams['savefig.dpi'] = 250\n",
    "# mpl.rcParams['figure.dpi'] = 250\n",
    "# mpl.rcParams['savefig.format'] = 'pdf'\n",
    "# mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from matplotlib.colors import LogNorm\n",
    "\n",
    "# import pylab\n",
    "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "# from mpl_toolkits.axes_grid1.inset_locator import inset_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {90: 'SNIa',\n",
    "              67: 'SNIa-91bg',\n",
    "              52: 'SNIax',\n",
    "              42: 'SNII',\n",
    "              62: 'SNIbc',\n",
    "              95: 'SLSN-I',\n",
    "              15: 'TDE',\n",
    "              64: 'KN',\n",
    "              88: 'AGN',\n",
    "              92: 'RRL',\n",
    "              65: 'M-dwarf',\n",
    "              16: 'EB',\n",
    "              53: 'Mira',\n",
    "              6: r'$\\mu$Lens-Single'}\n",
    "\n",
    "true_labels = label_dict.copy()\n",
    "true_labels[991] = r'$\\mu$Lens-Binary'\n",
    "true_labels[992] = 'ILOT'\n",
    "true_labels[993] = 'CaRT'\n",
    "true_labels[994] = 'PISN'\n",
    "true_labels[995] = r'$mu$Lens-String'\n",
    "\n",
    "sub_labels = label_dict.copy()\n",
    "sub_labels[99] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contestants = ['1_Kyle', '2_MikeSilogram', '3_MajorTom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 50\n",
    "positions = np.linspace(0., 1., nbins)\n",
    "dx = 1. / nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_target in label_dict.keys():\n",
    "    for cl_est in label_dict.keys():\n",
    "        for contestant in contestants:\n",
    "            thepath = os.path.join('submissions/'+contestant, 'violin'+str(one_target)+'true'+str(cl_est)+'pred.txt')\n",
    "            data = np.exp(np.genfromtxt(thepath))\n",
    "            if not np.all(~np.isinf(data)):\n",
    "                data[0] = 1. / dx\n",
    "                data[1:] = 0.\n",
    "            elif not np.all(data == 1.):\n",
    "                data = data / np.max(data)\n",
    "            else:\n",
    "                data = np.zeros_like(data)\n",
    "            print(\"I just read in \"+contestant+\"'s violin curve of true \"+label_dict[one_target]+\" assigned \"+sub_labels[cl_est])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar metrics of disparity between 1-D probability densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler divergence\n",
    "\n",
    "Of each classifier relative to a \"ground truth\" of `'1_Kyle'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quick_kld(p_eval, q_eval, dx=0.01):\n",
    "#     \"\"\"\n",
    "#     Calculates the Kullback-Leibler Divergence between two evaluations of PDFs.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     p_eval: numpy.ndarray, float\n",
    "#         evaluations of probability distribution whose distance _from_ `q` will be calculated\n",
    "#     q_eval: numpy.ndarray, float\n",
    "#         evaluations of probability distribution whose distance _to_ `p` will be calculated.\n",
    "#     dx: float\n",
    "#         resolution of integration grid\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     Dpq: float\n",
    "#         the value of the Kullback-Leibler Divergence from `q` to `p`\n",
    "#     \"\"\"\n",
    "#     logquotient = safelog(p_eval) - safelog(q_eval)\n",
    "#     # logp = safelog(pn)\n",
    "#     # logq = safelog(qn)\n",
    "#     # Calculate the KLD from q to p\n",
    "#     Dpq = dx * np.sum(p_eval * logquotient)\n",
    "#     return Dpq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE\n",
    "\n",
    "Between classifier pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quick_rmse(p_eval, q_eval, N):\n",
    "#     \"\"\"\n",
    "#     Calculates the Root Mean Square Error between two evaluations of PDFs.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     p_eval: numpy.ndarray, float\n",
    "#         evaluation of probability distribution function whose distance between\n",
    "#         its truth and the approximation of `q` will be calculated.\n",
    "#     q_eval: numpy.ndarray, float\n",
    "#         evaluation of probability distribution function whose distance between\n",
    "#         its approximation and the truth of `p` will be calculated.\n",
    "#     N: int\n",
    "#         number of points at which PDFs were evaluated\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     rms: float\n",
    "#         the value of the RMS error between `q` and `p`\n",
    "#     \"\"\"\n",
    "#     # Calculate the RMS between p and q\n",
    "#     rms = np.sqrt(np.sum((p_eval - q_eval) ** 2) / N)\n",
    "#     return rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein distance\n",
    "\n",
    "Between classifier pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing metrics\n",
    "\n",
    "Plot the per-true-class, per-assigned-class, per-classifier metrics as a per-classifier $N_{class} \\times N_{class}$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing metrics\n",
    "\n",
    "Combine per-true-class, per-assigned-class, per-classifier-pair metrics into one scalar value per pair of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLAsTiCC (Python 3)",
   "language": "python",
   "name": "plasticc_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
