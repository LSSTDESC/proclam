\section{Discussion}
\label{sec:discussion}

The goal of this work is to identify the metric most suited to \plasticc, which seeks classification posteriors of complete light curves similar to those anticipated from \lsst, with an emphasis on classification over all types, rewarding a ``best in show'' classifier rather than focusing on any one class or scientific application.\footnote{At the conclusion of \plasticc, other metrics specific to scientific uses of one or more particular classes will be used to identify ``best in class'' classification procedures that will be useful for more targeted science cases.}
The weighted log-loss is thus the metric most suited to the current \plasticc\ release.

\changes{Classification of transient and variable objects is important for a variety of scientific objectives. This diversity of scientific goals requires different trade-offs, and must be evaluated using multiple metrics. While we leave addressing such challenges for future releases of \plasticc\, and the identification of appropriate metrics to future work, we end by enumerating some of the science goals and approaches for identifying metrics.} 
\sout{Future releases of \plasticc\ will focus on different challenges in transient and variable object classification, with metrics appropriate to identifying methodologies that best enable those goals.
We discuss approaches to identifying optimal metrics for these variations, which may be developed further in future work.}

\subsection{Early classification}
\label{sec:early}

Spectroscopic follow-up is only expected of a small fraction of \lsst's detected transients and variable objects due to limited resources for such observations.
In addition to optical spectroscopic follow-up, photometric observations in other wavelength bands (near infrared and x-ray from space; microwave and radio from the ground) will be key to building a physical understanding of the object, particularly as we enter the era of multi-messenger astronomy with the added possibility of optical gravitational wave signatures.
Prompt follow-up observations are highly informative for fitting models to the light curves of familiar source classes and to characterizing anomalous light curves that could indicate never-before-seen classes that have eluded identification due to rarity or faintness.
As such, decisions about follow-up resource allocation must be made quickly and under the constraint that resources wasted on a misclassification consume the budget remaining for future follow-up attempts.
A future version of \plasticc\ focused on early light curve classification should have a metric that accounts for these limitations and rewards classifiers that perform better even when fewer observations of the lightcurve are available.

We consider the decision of whether to initiate follow-up observations to be binary and deterministic.
However, it is possible to conceive of non-binary decisions about follow-up resources; for example, one could choose between dedicating several hours on a spectroscopic instrument following up on one likely candidate or dedicating an hour each on several less likely candidates.
Here, we will discuss a metric for an early classification challenge to be focused on deterministic classification because the conversion between classification posteriors and decisions is uncharted territory that we do not explore at this time.

Even within the scope of spectroscopic follow-up as a primary motivation for early light curve classification, the goals of model-fitting to known classes and discovery of new classes would likely not share an optimal metric.
The critical question for choosing the most appropriate metric for any specific science goal motivating follow-up observations is to maximize information.
We provide two examples of the kind of information one must maximize via early light curve classification and the qualities of a deterministic metric that might enable it.

Supernova cosmology with spectroscopically confirmed light curves benefits from true positives, which contribute to the constraining power of the analysis by including one more data point;
when the class in which one is interested is as plentiful as SN Ia and our resources limited a priori, we may not be concerned by a high rate of false negatives.
% requires making a decision balancing the improved constraining power of including another SN Ia in the analysis, thereby constraining the cosmological parameters, so only true positives contribute information, and if we had a perfect classifier and standard follow-up spectroscopy resources, there would be a maximum amount of information about the cosmological parameters that could be gained in this way.
% Each false positive uses the same resources but adds no information about the cosmological parameters, and each false negative consumes no follow-up resources and deprives the Hubble diagram of one more data point.
False positives, on the other hand, may not enter the cosmology analysis, but they consume follow-up resources, thereby depriving the endeavor of the constraining power due to a single SN Ia.

A perfect classifier would lead to a maximum amount of information about the cosmological parameters conditioned on the follow-up resource budget.
For this scientific application, the metric must be chosen to not only maximize true positives but also to minimize false positives, and their relative impacts on the cosmological constraints can be quantified in terms of the information one would have about the cosmological parameters under different balances of true and false positives.
% balance the value of the information forgone by a false positive and the value of information forgone by a false negative, and the value placed on these is effectively weighted by the value we as researchers place on follow-up resources.
% \aim{Ciite some deterministic metrics relating to TP/FP?}

\subsection{Anomaly Detection}
\label{sec:anom}
Anomaly detection also gains information only from true positives, but the cost function is different in that the potential gain of information from a true positive, since there is no information about undiscovered classes ahead of time.
An example would be the recent detection of a kilonova, flagged initially by the detection of gravitational waves from an object.

Resource availability for identifying new classes is more flexible, increasing when new predictions or promising preliminary observations attract attention, and decreasing when a discovery is confirmed and the new class is established.
In this way, a false positive does not necessarily consume a resource that could otherwise be dedicated to a true positive, and the potential information gain is sufficiently great that additional resources would likely be allocated to observe the potential object.
% A false negative, on the other hand, represents forgoing an unbounded quantity of information, so minimizing the false negative rate is as important as maximizing the true positive rate.
% For a rare event like a kilonova, a false negative represents an unbounfalse positive does not appreciably reduce the amount of remaining information available to collect, but a false negative represents a large quantity of information forgone.
% Furthermore, r
% In this case, the information forgone by a false negative is significant compared to the information forgone by a false positive.
Thus, a metric tuned to anomaly detection would aim to minimize the false negative rate and maximize the true positive rate.
% \aim{Cite some deterministic metrics relating to TP/FN?}

% \subsection{Hierarchical classes}
% \label{sec:hierarchical}
%
% \aim{TODO: We would like to at some point add some content on possible ideas for extending metrics to hierarchical classes, namely conditional extensions of log-loss and possible drawbacks of penalization that can be compensated for by weighting, as well as the challenge that could pose for interpretation.}

\subsection{Difficult light curve classification}
\label{sec:difficult}

Photometric light curve classification may be challenging for a number of reasons, including the sparsity and irregularity of observations, the possible classes and how often they occur, and the distances and brightnesses of the sources of the light curves.
These factors may represent limitations on the information content of the light curves, but appropriate classifiers may be able to overcome them to a certain degree.

Though quality cuts can eliminate the most difficult light curves from entering samples used for science applications, such a practice discards information that may be of value under an analysis methodology leveraging the larger number of light curves included in a sample without cuts.
Thus, classification methods that perform well on light curves characterized by lower signal-to-noise ratios are specially important for exploiting the full potential of upcoming surveys like \lsst.

This version of \plasticc\ implements quality cuts to homogenize difficulty to some degree, and notions of classification difficulty may depend on information that will not be available until after the challenge concludes.
While the groundwork for a metric incorporating data quality has been laid by \citet{wu_radio_2018}, we defer to future work an investigation of this possibility.
