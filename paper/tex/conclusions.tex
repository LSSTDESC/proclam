\section{Conclusion}
\label{sec:conclusion}

As part of the preparation for \plasticc\, we investigate the properties of metrics suitable for probabilistic light curve classifications in the absence of a single scientific goal.
To that end, we sought a metric that avoids reducing classification probabilities to deterministic labels and one that rewards a classifier with strong performance across all classes over a classifier that performs well on a small subset of the classes and poorly on all others.

We compared two metrics specific to probabilistic classifications: the Brier score and the log-loss.
Even though the Brier score and log-loss metrics take values consistent with one another, they are structurally and conceptually different, with wholly different interpretations.
The Brier score is a sum of square differences between probabilities; the explicit penalty term is an attractive feature, but it treats probabilities as generic scores and is not interpretable in terms of information.
The log-loss on the other hand is readily interpretable, meaning the metric itself could be propagated into forecasting the constraining power of \lsst, affecting the choice of observing strategy.

We discovered that the log-loss is somewhat more sensitive to the systematic errors in classification that we find most concerning for generic scientific applications.
While both metrics could be appropriate for \plasticc, the log-loss is preferable due to its interpretability in terms of information.

Both metrics are susceptible to rewarding a classifier that performs well on the most prevalent class and poorly on all others, which fails to meet the needs of \plasticc's diverse motivations.
We explored a weighted average of the metric values on a per-class basis as a possible mitigation strategy to incentivize classifying uncommon classes, effectively ``leveling the playing field'' in the presence of highly imbalanced class membership.
Although weights do impact the interpretability of the log-loss, we select a per-class weighted log-loss as the optimal choice for \plasticc.

% We note that in order to map on to the Kaggle evaluation platform, a metric weighted only by class was used for the general challenge, while a log-loss with more complicated weighting procedure will be used for the science competition (which will continue for an additional month after the main Kaggle release).

We conclude by noting that care should be taken in planning future open challenges to ensure alignment between the challenge goals and the performance metric, so that efforts are best directed to achieve the challenge objectives.
We hope that this study of metric performance across a range of systematic effects and weights may serve as a guide to approaching the problem of identifying optimal probabilistic classifiers for general science applications.
