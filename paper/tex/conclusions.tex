\section{Conclusion}
\label{sec:conclusion}

As part of the preparation for \plasticc\, we investigate\changes{d} the properties of metrics suitable for probabilistic light curve classifications in the absence of a single scientific goal. Therefore, we sought a metric that avoids reducing classification probabilities to deterministic labels and \changes{one that works in a multi-class rather than a binary (two class) setting. In line with the goals of the \plasticc\ challenge, an important desideratum was to have a metric that tends to} reward a classifier's performance across all classes over a classifier that performs well on a small subset of the classes and poorly on all others. \changes{Given the potential of large class imbalance in astronomical datasets, we were also interested in the possibility of up-weighting the importance of certain rarer transient classes if need be; consequently we wanted to understand the way the metric would behave with the use of weights.}


\changes{To this end, in Sec.~\ref{sec:data}, we described a set of mock classifiers with characteristics that we expect some transient and variable classifiers to have, and designed methods to simulate submissions from such mock classifiers. This enabled us to test the performance of the metrics on each of these mock submissions.}

\sout{We compared two metrics specific to probabilistic classifications: the Brier score and the log-loss.}

\changes{To start with, we found two metrics within the literature : the Brier score and the log-loss metric that satisfied the criteria of working with probabilistic classifications in a multi-class problem. We left aside popular metrics (such as accuracy, AUC, the SPCC metrics) which did not satisfy these criteria, even though it might be possible to extend them for these scenarios.} The Brier score and the log-loss metrics are structurally and conceptually different, with wholly different interpretations. The Brier score is a sum of square differences between probabilities; the explicit penalty term is an attractive feature, but it treats probabilities as generic scores and is not interpretable in terms of information. The log-loss on the other hand is readily interpretable, meaning the metric itself could be propagated into forecasting the constraining power of \lsst, affecting the choice of observing strategy.


%%%% 
\changes{We evaluated these metrics using the mock submissions from the mock classification schemes with unit weights and then by varying the weights in Eqn.~\ref{eq:weightavg}. We found that both the Brier score and the log-loss behaved in an intuitive fashion on all of the mock classification results (see Table.~\ref{tab:extents}). Both metrics reward the best classifiers and penalize the worst (uncertain) classifers, but are susceptible to rewarding a classifier that performs well on the most prevalent class and poorly on all others, which fails to meet the needs of \plasticc's diverse motivations. For these cases, we discovered that the log-loss metric was somewhat more discerning in terms of metric values, but in terms of rankings both could be equivalent. Indeed, upto a monotonic transformation, their performances are quite similar.}
%%%% 
%%%% 
%%%% Even though the Brier score and log-loss metrics take values consistent with one another, they are structurally and conceptually different, with wholly different interpretations.
%%%% The Brier score is a sum of square differences between probabilities; the explicit penalty term is an attractive feature, but it treats probabilities as generic scores and is not interpretable in terms of information.
%%%% The log-loss on the other hand is readily interpretable, meaning the metric itself could be propagated into forecasting the constraining power of \lsst, affecting the choice of observing strategy.
%%%% 
%%%% We discovered that the log-loss is somewhat more sensitive to the systematic errors in classification that we find most concerning for generic scientific applications.
%%%% While both metrics could be appropriate for \plasticc, the log-loss is preferable due to its interpretability in terms of information.
%%%% 
%%%% Both metrics are susceptible to rewarding a classifier that performs well on the most prevalent class and poorly on all others, which fails to meet the needs of \plasticc's diverse motivations.

\changes{Due to our desire to potentially upweight transients,} We explored a weighted average of the metric values on a per-class basis as a possible mitigation strategy to incentivize classifying uncommon classes, effectively ``leveling the playing field'' in the presence of highly imbalanced class membership. %\changes{%Such weights were taken to be the same for all objects in the same class.
\changes{While modifyinging the log-loss metric to handle weights for different classes changes the interpretability, this would likely be appreciably different if the weights were significantly different at the level of orders of magnitude.
As it finally turned out, the weights actually used in \plasticc\ were all of similar orders, rather than different by orders of magnitude but this decision was concealed by design from some of the authors of this paper.} 

\changes{Given that both log-loss and Brier score seemed to work reasonably well for our requirements, we did not further try to define new metrics by extending known concepts. We chose the weighted log-losss metric because it seemed slightly more discerning, and easy to interpret in terms of information theory, at least in the limit of equal weights. It is probable, that the use of the Brier score would be similar as well.}  
%%%% Although weights do impact the interpretability of the log-loss, we select a per-class weighted log-loss as the optimal choice for \plasticc.
%%%% 
%%%% % We note that in order to map on to the Kaggle evaluation platform, a metric weighted only by class was used for the general challenge, while a log-loss with more complicated weighting procedure will be used for the science competition (which will continue for an additional month after the main Kaggle release). 

We conclude by noting that care should be taken in planning future open challenges to ensure alignment between the challenge goals and the performance metric, so that efforts are best directed to achieve the challenge objectives. We hope that this study of metric performance across a range of systematic effects and weights may serve as a guide to approaching the problem of identifying optimal probabilistic classifiers for general science applications.
