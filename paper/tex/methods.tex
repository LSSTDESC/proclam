\section{Methods}
\label{sec:methods}

%\begin{itemize}
%\item    The metric must return a single scalar value.
%\item    The metric must be well-defined for non-binary classes.
%\item    The metric must balance diverse science use cases in the presence of heavily nonuniform class prevalence.
%\item    The metric must respect the information content of probabilistic classifications.
%\item    The metric must be able to evaluate deterministic classifications.
%\item    The metric must be interpretable, meaning it gives a more optimal value for "good" mock classifiers and a less optimal value for mock classifiers plagued by anticipated systematic errors; in other words, it must pass basic tests of intuition.
%\item    The metric must be reliable, giving consistent results for different instantiations of the same test case.
%\end{itemize}

\plasticc\ aims to motivate development of improved techniques.
In order to discriminate between techniques, there must be a performance metric, a single scalar value quantifying how appropriate a classifier is for the task at hand.
Choosing a metric therefore is logically entwined with the question the challenge aims to answer.
The goal of \snphotcc\ was to identify good classifiers of SN Ia used for a photometric cosmology analysis, which lends itself to a metric that optimizes the purity of any resultant data set.
The issue of ``early classification'' for the purpose of decisionmaking regarding allocation of precious follow-up resources to spectroscopically confirm likely candidates was not addressed by the challenge participants, though it was (and remains) key to using spectroscopic resources wisely.
\plasticc, on the other hand, differs from \snphotcc\ in that its goals are not tied to one application or even one type of transient or variable class.
As such, the choice of a metric is not so simple, which has many diverse science goals with few arguments strong enough for meaningful fractions of spectroscopic follow-up.

The initial \plasticc\ challenge presents the entirety of the lightcurve for classification.
While classification of the earliest part of the lightcurve will be valued by the science team itself, the challenge is not overtly restrictive by giving classifiers only the first few points, something that will be included explicitly in upcoming versions of the challenge, to simulate the needs of early brokers for LSST.

%he posterior probabilities of class must be accurate for use in inference, and they must be evaluated in a way that does not excessively favor any one science application.

In Section~\ref{sec:science}, we review the metrics that were used in \snphotcc.
% In Section~\ref{sec:deterministic}, we review the metrics that were used in \snphotcc\ as the most similar previous challenge.
In Section~\ref{sec:probabilistic}, we introduce metrics appropriate for probabilistic classification.
We take weighted averages of the per-object metrics with per-class weights described in Section~\ref{sec:weights}.

\subsection{Science-motivated metrics}
\label{sec:science}

% TODO: Needs work to make consistent with rest of paper!

\snphotcc\ aimed to identify deterministic classifiers of SN Ia using a metric
\begin{eqnarray}
  \label{eq:snphotccfom}
  \mathcal{FOM} &\equiv& \epsilon_{Ia} \times \tilde{P}_{Ia}
\end{eqnarray}
which is the product of the \textit{efficiency}\footnote{Efficiency is generically defined as $\epsilon = \mathrm{TP} / (\mathrm{TP} + \mathrm{FN})$.}
\begin{eqnarray}
  \label{eq:efficiency}
  \epsilon_{Ia} &=& \frac{N_{Ia}^{\mathrm{true}}}{N_{Ia}^{TOT}}
\end{eqnarray}
of SN Ia classification and \textit{pseudo-purity}\footnote{Purity, also known as the positive predictive value, is generically defined as $P = \mathrm{TP} / (\mathrm{TP} + \mathrm{FP})$.}
\begin{eqnarray}
  \label{eq:pseudopurity}
  \tilde{P}_{Ia} &=& \frac{N_{Ia}^{\mathrm{true}}}{N_{Ia}^\mathrm{true} + W_{Ia}^\mathrm{false}N_{Ia}^\mathrm{false}}
\end{eqnarray}
with an additive penalty term with weight $W_{Ia}^\mathrm{false}$ motivated by the potential cosmology-parameter biases from  CC contamination.
The pseudo-purity can be interpreted as the traditional purity factor when $W_{Ia}^\mathrm{false} = 1$.

For the \snphotcc\ the penalty was related to the size of the spectroscopic subsample as roughly $W_{Ia}^\mathrm{false} = 1 + \epsilon_{spec}^{-1} \gg 1$ but the conservative limit of $W_{Ia}^\mathrm{false} = 3$ was chosen.
% to penalize wasted spectroscopic time over rejected SNe.
For future challenges, a more balanced metric can be used to ensure correct classifications across the range of classes, without focusing or highlighting a specific class as above.

% \subsection{Deterministic metrics}
% \label{sec:deterministic}

% TODO: write about AUC

\subsection{Probabilistic metrics}
\label{sec:probabilistic}

\plasticc\ aims to identify classifiers that produce discrete posterior probability distributions $p(m \mid d)$ over $M$ classes $m$ given their photometric lightcurves $d$, not deterministic classifications.
% The first difference will be discussed in Section~\ref{sec:weights}.
% TODO: discuss ROC/AUC, etc. here as well?
We consider two metrics of classification probabilities that avoid reducing probabilities to point estimates of class.
Both our metrics make use of the indicator variable
\begin{eqnarray}
  \label{eq:indicator}
  \tau_{n, m} &\equiv& \begin{cases}
  0 & m' \neq m\\
  1 & m' = m
  \end{cases}
\end{eqnarray}
for each possibile class $m$ for the $n^{\mathrm{th}}$ lightcurve which in truth belongs to class $m'$.
Both of our metrics are strictly nonnegative and approach zero for a truly perfect classifier.

\subsubsection{Log-loss}
\label{sec:logloss}

\aim{This section needs rewriting to remove jargon and clarify the discrepancies raised by Kaisey/Ashish/Tom.}
The log-loss (which is related to the cross-entropy as defined below) is a measure of how a classification algorithm performs in the case where the prediction input is a probability value between zero and unity.

\begin{eqnarray}
  \label{eq:entropy}
  H_{n} &=& -\sum_{m=1}^{M} \tau(m \mid d_{n}) \ln[\tau(m \mid d_{n})].
\end{eqnarray}
Hence $H_n$ is a measure of the entropy of the \textit{state space} of classes based on the lightcurve data, independent of knowledge of the true class. Any deterministic classification, regardless of accuracy, minimizes the entropy, and the uncertain classifier of Section~\ref{sec:uncertaindata} can be proven to maximize the entropy \citep{murphy_machine_2012}.
% TODO: fix this citation!
In an information theory context, the cross-entropy between two distributions (or in this case classes) over the same set of light curve data, describes the average number of bits needed to identify an event (lightcurve) from the set $d_n$, assuming a classification scheme is used that is optimized for an ``unnatural'' probability distribution $p$ rather than the true distribution $\tau_{n,m}$:
\begin{eqnarray}
  \label{eq:logloss}
  L_{n} &=& -\sum_{m=1}^{M}\tau_{n, m}\ln[p(m \mid d_{n})].
\end{eqnarray}
So $L_n$ measures the \textit{disorder} of using $p(m \mid d_{n})$ in place of $\tau_{n, m}$.
A difference between $L_{n}$ and $H_{n}$ evaluated at $\tau_{n}$ would be the information lost to disorder in using $p(m \mid d_{n})$ in place of $\tau_{n, m}$, also known as the Kullback-Leibler Divergence (KLD).
See \citet{malz_approximating_2018} for a comprehensive exploration of the KLD for a continuous 1-dimensional probability space.
The log-loss has only recently established a presence in the astronomy literature \citep{hon_deep_2017, hon_deep_2018}.
Its greatest strength is that it is straightforwardly interpretable, enabling the metric itself to directly contribute to uncertainty propagation in an inference problem using the probability densities provided by the classifier.
% \aim{[Rahul: Are you saying you could rewrite a BEAMs like model in terms of the logloss metric rather than the Probabilities P(m|d)?]}
% \aim{[Alex: Not rather than, but as the performance metric of how it's doing.  When propagated through a cosmology calculation, we'd be able to say that BEAMS improves the cosmological parameters by preserving X more information than the alternative.]}

\subsubsection{Brier score}
\label{sec:brier}

The Brier score \citep{brier_verification_1950}, given as
\begin{eqnarray}
  \label{eq:brier}
B_{n} &=& \sum_{m=1}^{M}(\tau_{n, m}-p(m \mid d_{n}))^{2}.
\end{eqnarray}
is a mean square error calculated between the true class indicator vector and the estimated probability vector.
It has been used extensively in solar flare forecasting \citep{crown_validation_2012, mays_ensemble_2015, florios_forecasting_2018}, stellar variability identification \citep{richards_construction_2012, armstrong_k2_2016}, and star-galaxy separation \citep{kim_hybrid_2015}.

The Brier score is an attractive option because it both rewards classifiers assigning high probability to the true class and penalizes classifiers for assigning high probability to classes other than the true class.
We expect this difference to significantly distinguish the Brier score from the log-loss.

However, its interpretation is less obvious, as its dimensions depend on those of the probability space upon which the posterior estimates are defined (classes, in the case of \plasticc).
Furthermore, modifying it with weights requires choosing whether to weight only per-object values $B_{n}$ or also the individual terms in the metric above.
We leave to future work the thorough investigation of a nontrivial weighting scheme on the Brier metric.

%\textbf{RH: this isnt super clear, we should discuss}
%\aim{I'm not sure if there's time to actually investigate the latter, nor how forthcoming to be about that being a nontrivial concern that drove the final decision about a metric for Kaggle.}

% \subsubsection{Conditional density estimation loss}
% \label{sec:cdeloss}
%
% \aim{The CDE Loss should also be included in the paper (and sent to Kaggle, if there's still time) if I can figure out how to make it work for non-ordered domains, i.e. classes.}

\subsection{Weights}
\label{sec:weights}

% TODO: motivate weights as opposed to flat
% TODO: rename section

The underlying causes of the most troubling of the systematics of Section~\ref{sec:mockdata}, tunnel vision and cruise control, are the nonrepresentative training set and an extreme imbalance of class membership rates.
Nonrepresentativity is a common problem in astronomy, as different objects naturally occur with different frequency in the universe.
Something that complicates matters even further is that classes often look alike in different scenarios.
For example, supernovae (stars that are completely destroyed in their explosions) often look like cataclysmic variables or CVs (stars that are not destroyed and whose brightness changes in a repeated way) given only the first few lightcurve points.
Similarly tidal disruption events (TDEs) that occur when stars get sufficiently close to the central black hole of a galaxy to be destroyed can look much like supernovae that are located close to the center of the galaxy.
Over longer periods of time, variations in the output from the central source of a galaxy, the active galactic nucleus (AGN) can mimic the output of a CV.

As mentioned in the previous section, subclasses often look similar but are useful in understanding very different science cases, so a metric that rewards a nuanced classifier is desireable.

\lsst\ will suffer from nonrepresentative training sets, an imbalance of classes and will see many subclasses.
These features are therefore present in \plasticc; class prevalence will differ by orders of magnitude, and there will be classes that are not present in the training set at all.
% This can be problematic because our science needs might require accurate classifications of uncommon classes as well.
Because tunnel vision is actually strong performance, it is common for tunnel classifiers to dominate challenges.
Under a ``winner takes all'' challenge and with equal weight per lightcurve, \plasticc\ would be particularly vulnerable to a tunnel vision classifier winning despite not meeting the needs of those studying less common classes.

One option is to apply a threshold of classification of all classes in order to assign an overall winner, though it would require reducing the classification probabilities to point estimates of class.
When doing binary classification with a method that reduces probabilities to point estimates of class, each object is assigned the class of higher probability, even if the two probabilities are quite similar.
This situation is particularly likely if the object, in fact, belongs to a third class or if the two classes are subclasses of a single physical phenomenon.
%We thus anticipate it to be a more severe issue for \plasticc\ and other realistic multi-class challenges as well as any challenges with multiple subclasses.

A simple reduction to a point estimate would in general be inappropriate but possibly salvageable with a threshold mechanism.
For example, requiring a minimum difference in probability density between the maximum probability class and the next highest probability class would help avert this degeneracy.
% (e.g. a newly discovered supernova with a very small number of points may be indistinguishable from a Cataclysmic variable going through a brightening).

The alternative we systematically investigate in this paper is to use a ``flat'' average of per-class metrics
\begin{eqnarray}
  \label{eq:perclassavg}
Q_{m} &=& \frac{1}{N_{M}}\sum_{n}Q_{n}
\end{eqnarray}
resulting from first averaging the metric values $Q_{n}$ for each member $n$ (i.e. each observed transient) of the class $m$ (e.g., AGN, SN Ia, RRLyrae in the astronomical example).
For this study, we consider a general weighted average
\begin{eqnarray}
  \label{eq:weightavg}
Q_{tot} &=& \frac{1}{\sum_{m}w_{m}}\sum_{m=1}^{M}w_{m}Q_{m}
\end{eqnarray}
of the per-class metrics.
Non-flat weights may be chosen to encourage challenge participants to direct more attention to classes with less active classification literature or those that have been historically more difficult to classify due to the concerning systematics.
The weights for the \plasticc\ metric, however, must be determined before the knowledge of which systematics affect which classes exists.
Because of this, the choice of weights is an inherently human problem dictated by the value placed on the scientific merits of knowledge of each class.
This paper, on the other hand, can only quantify the impact of weights in relation to the systematics.
